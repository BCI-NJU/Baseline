{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FBCNet + Xgboost, fbc充当特征提取器, Xgboost充当分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import transforms\n",
    "from fbc import FBCNet\n",
    "from eegDataset import eegDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 制作XGBoost数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_folder_npy_data(folder_path):\n",
    "    '''\n",
    "    读取lyh的数据\n",
    "    '''\n",
    "    data_dict = {}\n",
    "\n",
    "    # Filter only .npy files\n",
    "    npy_files = [f for f in os.listdir(folder_path) if f.endswith('.npy')]\n",
    "\n",
    "    # Load .npy files\n",
    "    for filename in npy_files:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        try:\n",
    "            # Attempt to load the file\n",
    "            content = np.load(file_path, allow_pickle=True)\n",
    "\n",
    "            # Print information about the loaded file\n",
    "            print(f\"Loaded {filename}: Shape={content.shape}, Dtype={content.dtype}\")\n",
    "            \n",
    "            # Add to the dictionary\n",
    "            data_dict[filename] = content\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Print an error message if loading fails\n",
    "            print(f\"Error loading {filename}: {e}\")\n",
    "\n",
    "    return data_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded right2_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded left1_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded nothing2_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded left3_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded right3_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded leg2_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded right_processed.npy: Shape=(15, 300000), Dtype=float64\n",
      "Loaded left2_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded left_processed.npy: Shape=(15, 300000), Dtype=float64\n",
      "Loaded nothing3_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded leg1_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded nothing1_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded nothing_processed.npy: Shape=(15, 300000), Dtype=float64\n",
      "Loaded leg3_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded leg_processed.npy: Shape=(15, 300000), Dtype=float64\n",
      "Loaded right1_orginal.npy: Shape=(100,), Dtype=object\n"
     ]
    }
   ],
   "source": [
    "lyh_data_path = \"Emotiv_dataloader-main/data\"\n",
    "data_dict = read_folder_npy_data(lyh_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\"nothing_processed.npy\", \"left_processed.npy\", \"right_processed.npy\", \"leg_processed.npy\"]\n",
    "train_data, train_labels, test_data, test_labels = [], [], [], []\n",
    "for key in keys:\n",
    "    d = [data_dict[key][0:14, i:i+1000] for i in range(0, data_dict[key].shape[1], 1000)]\n",
    "    train_data += d[:int(len(d) * 0.8)]\n",
    "    train_labels += [keys.index(key)] * len(d[:int(len(d) * 0.8)])\n",
    "    test_data += d[int(len(d) * 0.8):]\n",
    "    test_labels += [keys.index(key)] * len(d[int(len(d) * 0.8):])\n",
    "\n",
    "# 为了适配tranforms.py里的filterBank类\n",
    "train_data = [{'data': d} for d in train_data]\n",
    "test_data = [{'data': d} for d in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bandfilter!!!\n",
    "filterTransform = {'filterBank':{'filtBank':[[4,8],[8,12],[12,16],[16,20],[20,24], \\\n",
    "    [24,28],[28,32],[32,36],[36,40]],'fs':250, 'filtType':'filter'}}\n",
    "\n",
    "# Check and compose transforms\n",
    "if len(filterTransform) >1 :\n",
    "    transform = transforms.Compose([transforms.__dict__[key](**value) for key, value in filterTransform.items()])\n",
    "else:\n",
    "    transform = transforms.__dict__[list(filterTransform.keys())[0]](**filterTransform[list(filterTransform.keys())[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = eegDataset(train_data, train_labels, transform=transform)\n",
    "test_data = eegDataset(test_data, test_labels, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = [], [], [], []\n",
    "\n",
    "# fbc extractor, input shape: batch x 1 x chan x time x filterBand\n",
    "features_extractor = FBCNet(nChan=14, nTime=1000, nClass=4)\n",
    "# weight_path = os.getcwd() +\"/best_model_lyh_4cls.pth\"  \n",
    "# features_extractor.load_state_dict(torch.load(weight_path, map_location=torch.device('cpu'))[\"model_state_dict\"])\n",
    "\n",
    "for d in train_data:\n",
    "    i = d['data'].reshape(1, 1, 14, 1000, 9)\n",
    "    features = features_extractor(i)[1]\n",
    "    features = features.squeeze().flatten()\n",
    "    X_train.append(features.detach().numpy())\n",
    "    y_train.append(d['label'])\n",
    "\n",
    "for d in test_data:\n",
    "    i = d['data'].reshape(1, 1, 14, 1000, 9)\n",
    "    features = features_extractor(i)[1]\n",
    "    features = features.squeeze().flatten()\n",
    "    X_test.append(features.detach().numpy())\n",
    "    y_test.append(d['label'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打乱数据集\n",
    "\n",
    "combined_data = list(zip(X_train, y_train))\n",
    "random.shuffle(combined_data)\n",
    "X_train, y_train = zip(*combined_data)\n",
    "\n",
    "combined_data = list(zip(X_test, y_test))\n",
    "random.shuffle(combined_data)\n",
    "X_test, y_test = zip(*combined_data)\n",
    "\n",
    "X_train, y_train, X_test, y_test = np.array(X_train), np.array(y_train), np.array(X_test), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(960, 1152)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 训练\n",
    "### 2.1 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.37083333333333335)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xgboost train\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_model = xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# fbc extractor: best_model_lyh_4cls  + XGBoost\n",
    "y_train_pred = xgb_model.predict(X_train)\n",
    "acc_train = (y_train == y_train_pred).sum() / len(y_train)\n",
    "\n",
    "y_test_pred = xgb_model.predict(X_test)\n",
    "acc_test = (y_test == y_test_pred).sum() / len(y_test)\n",
    "acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.35)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model = XGBClassifier()\n",
    "xgb_model = xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# fbc extractor: random init  + XGBoost\n",
    "y_train_pred = xgb_model.predict(X_train)\n",
    "acc_train = (y_train == y_train_pred).sum() / len(y_train)\n",
    "\n",
    "y_test_pred = xgb_model.predict(X_test)\n",
    "acc_test = (y_test == y_test_pred).sum() / len(y_test)\n",
    "acc_train, acc_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb_params = {\n",
    "#         'boosting_type': 'gbdt',\n",
    "#         'objective': 'regression',\n",
    "#         'metric': 'mae',\n",
    "#         'min_child_weight': 5,\n",
    "#         'num_leaves': 2 ** 5,\n",
    "#         'lambda_l2': 10,\n",
    "#         'feature_fraction': 0.8,\n",
    "#         'bagging_fraction': 0.8,\n",
    "#         'bagging_freq': 4,\n",
    "#         'learning_rate': 0.05,\n",
    "#         'seed': 2023,\n",
    "#         'nthread' : 16,\n",
    "#         'verbose' : -1,\n",
    "#     }\n",
    "\n",
    "# no_info = lgb.callback.log_evaluation(period=-1) # 禁用训练日志输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb_train = lgb.Dataset(X_train, label=y_train)\n",
    "# lgb_test = lgb.Dataset(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb_model = lgb.train(lgb_params, lgb_train, 200, valid_sets=lgb_test, callbacks=[no_info])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.35)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_model = LGBMClassifier()\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "# fbc extractor: best_model_lyh_4cls  + LGBMClassifier\n",
    "y_train_pred = lgb_model.predict(X_train)\n",
    "acc_train = (y_train == y_train_pred).sum() / len(y_train)\n",
    "\n",
    "y_test_pred = lgb_model.predict(X_test)\n",
    "acc_test = (y_test == y_test_pred).sum() / len(y_test)\n",
    "acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.35)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_model = LGBMClassifier()\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "# fbc extractor: random init  + LightGBM\n",
    "y_train_pred = lgb_model.predict(X_train)\n",
    "acc_train = (y_train == y_train_pred).sum() / len(y_train)\n",
    "\n",
    "y_test_pred = lgb_model.predict(X_test)\n",
    "acc_test = (y_test == y_test_pred).sum() / len(y_test)\n",
    "acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fbc extractor: best_model_lyh_4cls  + lightgbm\n",
    "# y_train_pred = lgb_model.predict(X_train, num_iteration=lgb_model.best_iteration)\n",
    "# acc_train = (y_train == np.round(y_train_pred)).sum() / len(y_train)\n",
    "\n",
    "# y_test_pred = lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)\n",
    "# acc_test = (y_test == np.round(y_test_pred)).sum() / len(y_test)\n",
    "# acc_train, acc_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   25.0s\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done 324 out of 324 | elapsed: 10.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n"
     ]
    }
   ],
   "source": [
    "## 定义参数取值范围\n",
    "learning_rate = [0.1, 0.3, 0.6]\n",
    "feature_fraction = [0.5, 0.8, 1]\n",
    "num_leaves = [16, 32, 64]\n",
    "max_depth = [-1,3,5,8]\n",
    "\n",
    "parameters = { 'learning_rate': learning_rate,\n",
    "              'feature_fraction':feature_fraction,\n",
    "              'num_leaves': num_leaves,\n",
    "              'max_depth': max_depth}\n",
    "model = LGBMClassifier(n_estimators = 50)\n",
    "\n",
    "## 进行网格搜索\n",
    "clf = GridSearchCV(model, parameters, cv=3, scoring='accuracy',verbose=3, n_jobs=-1)\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_fraction': 0.5,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_depth': -1,\n",
       " 'num_leaves': 16}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.37083333333333335)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fbc extractor: best_model_lyh_4cls  + LGBMClassifier(GridSearchCV调参)\n",
    "y_train_pred = clf.predict(X_train)\n",
    "acc_train = (y_train == y_train_pred).sum() / len(y_train)\n",
    "\n",
    "y_test_pred = clf.predict(X_test)\n",
    "acc_test = (y_test == y_test_pred).sum() / len(y_test)\n",
    "acc_train, acc_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
