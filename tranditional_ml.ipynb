{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 传统机器学习方法\n",
    "路线：前置准备 预处理 可视化 特征提取 特征选择 分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T01:48:13.995435Z",
     "iopub.status.busy": "2024-03-12T01:48:13.995059Z",
     "iopub.status.idle": "2024-03-12T01:48:15.048262Z",
     "shell.execute_reply": "2024-03-12T01:48:15.047633Z",
     "shell.execute_reply.started": "2024-03-12T01:48:13.995409Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import butter, filtfilt, iirnotch, lfilter\n",
    "from copy import deepcopy\n",
    "from entropy import *\n",
    "import entropy\n",
    "from load_lyh_data import  *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 前置准备\n",
    "加载lyh的数据，这里仅仅是加载数据对数据进行分割，并未对数据进行任何处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:53:46.128429Z",
     "iopub.status.busy": "2024-02-28T11:53:46.127914Z",
     "iopub.status.idle": "2024-02-28T11:53:49.707432Z",
     "shell.execute_reply": "2024-02-28T11:53:49.706812Z",
     "shell.execute_reply.started": "2024-02-28T11:53:46.128402Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded nothing1_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded nothing2_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded nothing3_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded left1_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded left2_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded left3_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded right1_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded right2_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded right3_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded leg1_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded leg2_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded leg3_orginal.npy: Shape=(100,), Dtype=object\n"
     ]
    }
   ],
   "source": [
    "raw_data_dict = read_folder_npy_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:53:49.724802Z",
     "iopub.status.busy": "2024-02-28T11:53:49.724499Z",
     "iopub.status.idle": "2024-02-28T11:53:49.729035Z",
     "shell.execute_reply": "2024-02-28T11:53:49.728482Z",
     "shell.execute_reply.started": "2024-02-28T11:53:49.724784Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 参数设计\n",
    "\n",
    "# 下采样设置：ERP脑电有效频率<30Hz，根据脑奎斯特定理，\n",
    "# 采样率到60Hz以上即可。但为了多一些保存细节，这里降采样设置为125Hz\n",
    "fs_raw = 250\n",
    "fs_down = 125\n",
    "# 一个trial的时间\n",
    "t_of_trial = 4\n",
    "# 采样点个数\n",
    "nTime = fs_down * t_of_trial\n",
    "\n",
    "start_point_index = 2\n",
    "end_point_index = start_point_index + fs_raw * t_of_trial\n",
    "\n",
    "\n",
    "def down_sample(data, start, end, fs_raw, fs_down):\n",
    "    '''\n",
    "    下采样\n",
    "    param---\n",
    "    data: numpy数组\n",
    "    return---\n",
    "    data: 下采样之后的数组\n",
    "    '''\n",
    "    \n",
    "    step = fs_raw // fs_down\n",
    "    data = data[start: end: step]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:53:49.736231Z",
     "iopub.status.busy": "2024-02-28T11:53:49.735902Z",
     "iopub.status.idle": "2024-02-28T11:53:49.739051Z",
     "shell.execute_reply": "2024-02-28T11:53:49.738505Z",
     "shell.execute_reply.started": "2024-02-28T11:53:49.736213Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 选择channel\n",
    "channel_index_list = np.arange(2, 2+14)\n",
    "\n",
    "def selected_channel(data, channel_index_list):\n",
    "\n",
    "    return data[:, channel_index_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-02-28T11:53:49.744805Z",
     "iopub.status.busy": "2024-02-28T11:53:49.744462Z",
     "iopub.status.idle": "2024-02-28T11:53:49.750076Z",
     "shell.execute_reply": "2024-02-28T11:53:49.749539Z",
     "shell.execute_reply.started": "2024-02-28T11:53:49.744786Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_label_dict = {\n",
    "    \"nothing1_orginal.npy\": 0, \"nothing2_orginal.npy\": 0, \"nothing3_orginal.npy\": 0,\n",
    "    \"left1_orginal.npy\": 1, \"left2_orginal.npy\": 1, \"left3_orginal.npy\": 1,\n",
    "    \"right1_orginal.npy\": 2, \"right2_orginal.npy\": 2, \"right3_orginal.npy\": 2,\n",
    "    \"leg1_orginal.npy\": 3, \"leg2_orginal.npy\": 3, \"leg3_orginal.npy\": 3\n",
    "}\n",
    "save_path = os.getcwd() + \"/lyh_data/session1/Standard_input\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "def pre_preparation(data_dict, file_label_dict, save_path, nClasses=4):\n",
    "    data = [[] for _ in range(nClasses)]\n",
    "\n",
    "    for key, value in data_dict.items():\n",
    "        d = data[file_label_dict[key]]\n",
    "        for trial in value:\n",
    "            trial = down_sample(np.array(trial), start_point_index, end_point_index, fs_raw, fs_down)\n",
    "            trail = selected_channel(trial, channel_index_list)\n",
    "            d.append(trail.T)\n",
    "    \n",
    "    # save\n",
    "    for i, d in enumerate(data):\n",
    "        data[i] = np.array(d)\n",
    "        path = save_path + \"/\" + str(i) + \".npy\"\n",
    "        np.save(path, data[i])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:53:49.751042Z",
     "iopub.status.busy": "2024-02-28T11:53:49.750780Z",
     "iopub.status.idle": "2024-02-28T11:53:51.173271Z",
     "shell.execute_reply": "2024-02-28T11:53:51.172608Z",
     "shell.execute_reply.started": "2024-02-28T11:53:49.751025Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "standard_input_data_list = pre_preparation(raw_data_dict, file_label_dict, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:53:51.174643Z",
     "iopub.status.busy": "2024-02-28T11:53:51.174414Z",
     "iopub.status.idle": "2024-02-28T11:53:51.178529Z",
     "shell.execute_reply": "2024-02-28T11:53:51.178024Z",
     "shell.execute_reply.started": "2024-02-28T11:53:51.174624Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 14, 500)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_input_data_list[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:53:51.179737Z",
     "iopub.status.busy": "2024-02-28T11:53:51.179373Z",
     "iopub.status.idle": "2024-02-28T11:53:51.183229Z",
     "shell.execute_reply": "2024-02-28T11:53:51.182712Z",
     "shell.execute_reply.started": "2024-02-28T11:53:51.179718Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 基线校正\n",
    "def baseline_correction(data, baseline_start, baseline_end):\n",
    "    \"\"\"\n",
    "    进行基线校正\n",
    "\n",
    "    Parameters:\n",
    "    - data: EEG 数据的二维数组，shape=(通道数, 采样点数)\n",
    "    - baseline_start: 基线起始点的索引\n",
    "    - baseline_end: 基线结束点的索引\n",
    "\n",
    "    Returns:\n",
    "    - baseline_corrected_data: 基线校正后的 EEG 数据\n",
    "    \"\"\"\n",
    "    # 计算每个通道上基线期的平均值\n",
    "    baseline_values = np.mean(data[:, baseline_start:baseline_end], axis=1, keepdims=True)\n",
    "\n",
    "    # 进行基线校正\n",
    "    baseline_corrected_data = data - baseline_values\n",
    "\n",
    "    return baseline_corrected_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-28T12:03:10.435862Z",
     "iopub.status.idle": "2024-02-28T12:03:10.436340Z",
     "shell.execute_reply": "2024-02-28T12:03:10.436225Z",
     "shell.execute_reply.started": "2024-02-28T12:03:10.436213Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基线校正前 :  [[4298.333 4301.538 4307.564 ... 4320.    4332.436 4314.872]\n",
      " [4268.462 4269.103 4268.462 ... 4282.821 4301.41  4287.051]\n",
      " [4328.462 4324.103 4328.333 ... 4351.795 4355.897 4346.667]\n",
      " ...\n",
      " [4463.077 4457.821 4469.615 ... 4502.308 4510.128 4505.897]\n",
      " [4335.641 4342.179 4357.308 ... 4426.795 4437.692 4419.231]\n",
      " [4418.846 4415.769 4421.154 ... 4469.615 4474.231 4469.744]]\n",
      "基线校正后 :  [[ 11.42274  14.62774  20.65374 ...  33.08974  45.52574  27.96174]\n",
      " [ 23.91832  24.55932  23.91832 ...  38.27732  56.86632  42.50732]\n",
      " [ 14.37734  10.01834  14.24834 ...  37.71034  41.81234  32.58234]\n",
      " ...\n",
      " [  0.56158  -4.69442   7.09958 ...  39.79258  47.61258  43.38158]\n",
      " [-26.3051  -19.7671   -4.6381  ...  64.8489   75.7459   57.2849 ]\n",
      " [-10.63602 -13.71302  -8.32802 ...  40.13298  44.74898  40.26198]]\n"
     ]
    }
   ],
   "source": [
    "# 作为基线的比例\n",
    "baseline_ratio = 0.1\n",
    "baseline_start = 0\n",
    "baseline_end = int(0.1 * nTime)\n",
    "\n",
    "x = deepcopy(standard_input_data_list[0][0])\n",
    "print(\"基线校正前 : \", x)\n",
    "x = baseline_correction(x, baseline_start, baseline_end)\n",
    "print(\"基线校正后 : \", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4298.333, 4301.538, 4307.564, ..., 4320.   , 4332.436, 4314.872],\n",
       "       [4268.462, 4269.103, 4268.462, ..., 4282.821, 4301.41 , 4287.051],\n",
       "       [4328.462, 4324.103, 4328.333, ..., 4351.795, 4355.897, 4346.667],\n",
       "       ...,\n",
       "       [4463.077, 4457.821, 4469.615, ..., 4502.308, 4510.128, 4505.897],\n",
       "       [4335.641, 4342.179, 4357.308, ..., 4426.795, 4437.692, 4419.231],\n",
       "       [4418.846, 4415.769, 4421.154, ..., 4469.615, 4474.231, 4469.744]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_input_data_list[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:53:51.192384Z",
     "iopub.status.busy": "2024-02-28T11:53:51.192187Z",
     "iopub.status.idle": "2024-02-28T11:53:51.197048Z",
     "shell.execute_reply": "2024-02-28T11:53:51.196538Z",
     "shell.execute_reply.started": "2024-02-28T11:53:51.192366Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 带通滤波器 0.05Hz-40Hz\n",
    "lowcut = 0.05\n",
    "highcut = 40\n",
    "fs = fs_down\n",
    "order = 2\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order):\n",
    "    fa = 0.5 * fs\n",
    "    low = lowcut / fa\n",
    "    high = highcut / fa\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    ret = []\n",
    "    for line in data:\n",
    "        ret.append(filtfilt(b, a, line))\n",
    "    return np.array(ret)\n",
    "\n",
    "def iirnotch_filter(data, fs = 125, Q = 30, f_cut = 50.0):\n",
    "    ret = []\n",
    "    b, a = iirnotch(f_cut, Q, fs)\n",
    "    for line in data:\n",
    "        ret.append(lfilter(b,a, line))\n",
    "    return np.array(ret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw data:  [[0.71975731 0.76830397 0.33347613 ... 0.96665006 0.98089686 0.64238853]\n",
      " [0.45019288 0.52688822 0.97496468 ... 0.50036375 0.93432375 0.69061286]\n",
      " [0.20299125 0.19351531 0.47885269 ... 0.82760863 0.10179565 0.3866357 ]\n",
      " ...\n",
      " [0.91063828 0.81434754 0.39893596 ... 0.46398696 0.54912456 0.45694705]\n",
      " [0.93614434 0.31974904 0.90733623 ... 0.69821682 0.51476252 0.34842152]\n",
      " [0.31800987 0.81562617 0.25448075 ... 0.86655682 0.70891056 0.33324974]]\n",
      "滤波:  [[-0.02321653 -0.01432984 -0.40747343 ...  0.03989705  0.27863996\n",
      "  -0.09407378]\n",
      " [ 0.0481928   0.21649998  0.43359855 ... -0.63016849 -0.15558356\n",
      "  -0.3685327 ]\n",
      " [ 0.06613227  0.14469515  0.20909337 ...  0.31116587  0.12106863\n",
      "   0.13481327]\n",
      " ...\n",
      " [ 0.08135337 -0.01291482 -0.42700086 ...  0.00525067  0.21542023\n",
      "   0.13344994]\n",
      " [-0.04829846 -0.42519548 -0.31395974 ...  0.63668495  0.39321582\n",
      "   0.22054132]\n",
      " [-0.16162607  0.17503425 -0.10593489 ...  0.86422312  0.73232321\n",
      "   0.37139722]]\n",
      "基线校正:  [[ 0.18717069  0.23571736 -0.19911049 ...  0.43406345  0.44831024\n",
      "   0.10980191]\n",
      " [ 0.0075683   0.08426365  0.53234011 ...  0.05773918  0.49169917\n",
      "   0.24798829]\n",
      " [-0.32173217 -0.33120811 -0.04587073 ...  0.30288521 -0.42292777\n",
      "  -0.13808772]\n",
      " ...\n",
      " [ 0.42380481  0.32751407 -0.08789751 ... -0.02284651  0.06229109\n",
      "  -0.02988642]\n",
      " [ 0.42991284 -0.18648245  0.40110473 ...  0.19198532  0.00853102\n",
      "  -0.15780998]\n",
      " [-0.14749627  0.35012003 -0.2110254  ...  0.40105068  0.24340442\n",
      "  -0.1322564 ]]\n",
      "基线校正+滤波:  [[-0.02321653 -0.01432984 -0.40747343 ...  0.03989705  0.27863996\n",
      "  -0.09407378]\n",
      " [ 0.0481928   0.21649998  0.43359855 ... -0.63016849 -0.15558356\n",
      "  -0.3685327 ]\n",
      " [ 0.06613227  0.14469515  0.20909337 ...  0.31116587  0.12106863\n",
      "   0.13481327]\n",
      " ...\n",
      " [ 0.08135337 -0.01291482 -0.42700086 ...  0.00525067  0.21542023\n",
      "   0.13344994]\n",
      " [-0.04829846 -0.42519548 -0.31395974 ...  0.63668495  0.39321582\n",
      "   0.22054132]\n",
      " [-0.16162607  0.17503425 -0.10593489 ...  0.86422312  0.73232321\n",
      "   0.37139722]]\n"
     ]
    }
   ],
   "source": [
    "# shape = (14, 500)  # 示例形状，这里是3行2列\n",
    "# test1 = np.random.rand(*shape)  # 解包shape为独立的参数\n",
    "# test2 = deepcopy(test1)\n",
    "# print(\"raw data: \", test1)\n",
    "# test1 = butter_bandpass_filter(test1, lowcut, highcut, fs, order)\n",
    "# test1 = iirnotch_filter(test1)\n",
    "# print(\"滤波: \", test1)\n",
    "# test2 = baseline_correction(test2, baseline_start, baseline_end)\n",
    "# print(\"基线校正: \", test2)\n",
    "# test2 = butter_bandpass_filter(test2, lowcut, highcut, fs, order)\n",
    "# test2 = iirnotch_filter(test2)\n",
    "# print(\"基线校正+滤波: \", test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "raw data:  [[0.01987379 0.34003506 0.05323721 ... 0.16883784 0.30067503 0.58919963]\n",
      " [0.31803727 0.05371726 0.65359817 ... 0.86014722 0.42232539 0.48166455]\n",
      " [0.26201529 0.80920886 0.10913765 ... 0.59461616 0.89124159 0.66696895]\n",
      " ...\n",
      " [0.82313029 0.19186278 0.83968552 ... 0.23007594 0.76695717 0.48286783]\n",
      " [0.72648586 0.84872972 0.0192662  ... 0.65085979 0.48650704 0.49081444]\n",
      " [0.11020996 0.1295229  0.50470722 ... 0.81128504 0.33153466 0.00380652]]\n",
      "滤波:  [[ 0.03289372  0.27890839  0.16071841 ... -0.28102633 -0.03514843\n",
      "   0.20150956]\n",
      " [ 0.18461827  0.10663454  0.25867693 ...  0.44846537  0.40118385\n",
      "   0.31161746]\n",
      " [-0.18206549  0.13941995 -0.12796279 ... -0.17973037  0.14465595\n",
      "  -0.05966101]\n",
      " ...\n",
      " [ 0.09933024 -0.28662583 -0.16634554 ... -0.36949289 -0.02220749\n",
      "  -0.17792545]\n",
      " [ 0.2084953   0.14098994 -0.20768738 ...  0.03845517 -0.13559455\n",
      "  -0.15714414]\n",
      " [ 0.00142415  0.09497048  0.22805068 ...  0.95522611  0.73574863\n",
      "   0.30518601]]\n",
      "基线校正:  [[-0.98012621 -0.65996494 -0.94676279 ... -0.83116216 -0.69932497\n",
      "  -0.41080037]\n",
      " [-0.68196273 -0.94628274 -0.34640183 ... -0.13985278 -0.57767461\n",
      "  -0.51833545]\n",
      " [-0.73798471 -0.19079114 -0.89086235 ... -0.40538384 -0.10875841\n",
      "  -0.33303105]\n",
      " ...\n",
      " [-0.17686971 -0.80813722 -0.16031448 ... -0.76992406 -0.23304283\n",
      "  -0.51713217]\n",
      " [-0.27351414 -0.15127028 -0.9807338  ... -0.34914021 -0.51349296\n",
      "  -0.50918556]\n",
      " [-0.88979004 -0.8704771  -0.49529278 ... -0.18871496 -0.66846534\n",
      "  -0.99619348]]\n",
      "基线校正+滤波:  [[ 0.03289372  0.27890839  0.16071841 ... -0.28102633 -0.03514843\n",
      "   0.20150956]\n",
      " [ 0.18461827  0.10663454  0.25867693 ...  0.44846537  0.40118385\n",
      "   0.31161746]\n",
      " [-0.18206549  0.13941995 -0.12796279 ... -0.17973037  0.14465595\n",
      "  -0.05966101]\n",
      " ...\n",
      " [ 0.09933024 -0.28662583 -0.16634554 ... -0.36949289 -0.02220749\n",
      "  -0.17792545]\n",
      " [ 0.2084953   0.14098994 -0.20768738 ...  0.03845517 -0.13559455\n",
      "  -0.15714414]\n",
      " [ 0.00142415  0.09497048  0.22805068 ...  0.95522611  0.73574863\n",
      "   0.30518601]]\n"
     ]
    }
   ],
   "source": [
    "# shape = (14, 500)  # 示例形状，这里是3行2列\n",
    "# test1 = np.random.rand(*shape)  # 解包shape为独立的参数\n",
    "# test2 = deepcopy(test1)\n",
    "# shape1 = (14, 1)\n",
    "# sub = np.ones(shape1)\n",
    "# print(sub)\n",
    "# print(\"raw data: \", test1)\n",
    "# test1 = butter_bandpass_filter(test1, lowcut, highcut, fs, order)\n",
    "# test1 = iirnotch_filter(test1)\n",
    "# print(\"滤波: \", test1)\n",
    "# test2 = test2 - sub\n",
    "# print(\"基线校正: \", test2)\n",
    "# test2 = butter_bandpass_filter(test2, lowcut, highcut, fs, order)\n",
    "# test2 = iirnotch_filter(test2)\n",
    "# print(\"基线校正+滤波: \", test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-02-28T11:53:51.198156Z",
     "iopub.status.busy": "2024-02-28T11:53:51.197828Z",
     "iopub.status.idle": "2024-02-28T11:53:51.210732Z",
     "shell.execute_reply": "2024-02-28T11:53:51.210206Z",
     "shell.execute_reply.started": "2024-02-28T11:53:51.198140Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "滤波前 :  [[ 11.42274  14.62774  20.65374 ...  33.08974  45.52574  27.96174]\n",
      " [ 23.91832  24.55932  23.91832 ...  38.27732  56.86632  42.50732]\n",
      " [ 14.37734  10.01834  14.24834 ...  37.71034  41.81234  32.58234]\n",
      " ...\n",
      " [  0.56158  -4.69442   7.09958 ...  39.79258  47.61258  43.38158]\n",
      " [-26.3051  -19.7671   -4.6381  ...  64.8489   75.7459   57.2849 ]\n",
      " [-10.63602 -13.71302  -8.32802 ...  40.13298  44.74898  40.26198]]\n",
      "基线矫正+滤波后 :  [[  1.2233674    5.31136275   9.38478621 ...  23.26321155  28.70143919\n",
      "   16.27871029]\n",
      " [  4.62252065   4.89188273   5.60975696 ...   7.82624006  23.85918253\n",
      "   12.7586109 ]\n",
      " [ -1.11384209  -4.53790032  -1.92974007 ...  11.24382281  13.61377979\n",
      "    5.92651266]\n",
      " ...\n",
      " [  0.54598765  -3.07385892   6.49437274 ...  -7.09438025  -0.34693384\n",
      "   -3.45731556]\n",
      " [-11.19020058  -4.24634911   8.3426814  ...  37.09434085  42.91027207\n",
      "   28.24304665]\n",
      " [ -4.26287477  -6.92866165  -2.46119829 ...   3.37859591   5.17940678\n",
      "    2.14527731]]\n"
     ]
    }
   ],
   "source": [
    "print(\"滤波前 : \", x)\n",
    "x = butter_bandpass_filter(x, lowcut, highcut, fs, order)\n",
    "x = iirnotch_filter(x)\n",
    "print(\"基线矫正+滤波后 : \", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "滤波前 :  [[4298.333 4301.538 4307.564 ... 4320.    4332.436 4314.872]\n",
      " [4268.462 4269.103 4268.462 ... 4282.821 4301.41  4287.051]\n",
      " [4328.462 4324.103 4328.333 ... 4351.795 4355.897 4346.667]\n",
      " ...\n",
      " [4463.077 4457.821 4469.615 ... 4502.308 4510.128 4505.897]\n",
      " [4335.641 4342.179 4357.308 ... 4426.795 4437.692 4419.231]\n",
      " [4418.846 4415.769 4421.154 ... 4469.615 4474.231 4469.744]]\n",
      "滤波后 :  [[  1.2233674    5.31136275   9.38478621 ...  23.26321155  28.70143919\n",
      "   16.27871029]\n",
      " [  4.62252064   4.89188273   5.60975696 ...   7.82624006  23.85918253\n",
      "   12.7586109 ]\n",
      " [ -1.11384209  -4.53790032  -1.92974007 ...  11.24382281  13.61377978\n",
      "    5.92651266]\n",
      " ...\n",
      " [  0.54598765  -3.07385893   6.49437274 ...  -7.09438025  -0.34693384\n",
      "   -3.45731556]\n",
      " [-11.19020058  -4.24634911   8.3426814  ...  37.09434085  42.91027207\n",
      "   28.24304665]\n",
      " [ -4.26287477  -6.92866165  -2.46119829 ...   3.37859591   5.17940678\n",
      "    2.14527731]]\n"
     ]
    }
   ],
   "source": [
    "print(\"滤波前 : \", standard_input_data_list[0][0])\n",
    "y = butter_bandpass_filter(standard_input_data_list[0][0], lowcut, highcut, fs, order)\n",
    "y = iirnotch_filter(y)\n",
    "print(\"滤波后 : \", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-02-28T11:53:51.211714Z",
     "iopub.status.busy": "2024-02-28T11:53:51.211356Z",
     "iopub.status.idle": "2024-02-28T11:53:51.215908Z",
     "shell.execute_reply": "2024-02-28T11:53:51.215419Z",
     "shell.execute_reply.started": "2024-02-28T11:53:51.211672Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_path = os.getcwd() + \"/lyh_data/session1/Preprocessed_data\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "def pre_processing(data, save_path, butter_order = 2):\n",
    "\n",
    "    for data_per_class in data:\n",
    "        for i, d in enumerate(data_per_class):\n",
    "            assert d.shape == (14, 500)\n",
    "            # data_per_class[i] = baseline_correction(data_per_class[i], baseline_start, baseline_end)\n",
    "            data_per_class[i] = butter_bandpass_filter(data_per_class[i], lowcut, highcut, fs, order)\n",
    "            data_per_class[i] = iirnotch_filter(data_per_class[i])\n",
    "    \n",
    "    # save\n",
    "    for i, d in enumerate(data):\n",
    "        data[i] = np.array(d)\n",
    "        path = save_path + \"/\" + str(i) + \".npy\"\n",
    "        np.save(path, data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:53:51.216981Z",
     "iopub.status.busy": "2024-02-28T11:53:51.216802Z",
     "iopub.status.idle": "2024-02-28T11:53:53.242558Z",
     "shell.execute_reply": "2024-02-28T11:53:53.241817Z",
     "shell.execute_reply.started": "2024-02-28T11:53:51.216965Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "滤波前： [[4298.333 4301.538 4307.564 ... 4320.    4332.436 4314.872]\n",
      " [4268.462 4269.103 4268.462 ... 4282.821 4301.41  4287.051]\n",
      " [4328.462 4324.103 4328.333 ... 4351.795 4355.897 4346.667]\n",
      " ...\n",
      " [4463.077 4457.821 4469.615 ... 4502.308 4510.128 4505.897]\n",
      " [4335.641 4342.179 4357.308 ... 4426.795 4437.692 4419.231]\n",
      " [4418.846 4415.769 4421.154 ... 4469.615 4474.231 4469.744]]\n",
      "滤波后:  [[  1.2233674    5.31136275   9.38478621 ...  23.26321155  28.70143919\n",
      "   16.27871029]\n",
      " [  4.62252064   4.89188273   5.60975696 ...   7.82624006  23.85918253\n",
      "   12.7586109 ]\n",
      " [ -1.11384209  -4.53790032  -1.92974007 ...  11.24382281  13.61377978\n",
      "    5.92651266]\n",
      " ...\n",
      " [  0.54598765  -3.07385893   6.49437274 ...  -7.09438025  -0.34693384\n",
      "   -3.45731556]\n",
      " [-11.19020058  -4.24634911   8.3426814  ...  37.09434085  42.91027207\n",
      "   28.24304665]\n",
      " [ -4.26287477  -6.92866165  -2.46119829 ...   3.37859591   5.17940678\n",
      "    2.14527731]]\n"
     ]
    }
   ],
   "source": [
    "print(\"滤波前：\", standard_input_data_list[0][0])\n",
    "pre_processing(standard_input_data_list, save_path)\n",
    "print(\"滤波后: \", standard_input_data_list[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 特征提取\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 时域特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:53:53.244306Z",
     "iopub.status.busy": "2024-02-28T11:53:53.243708Z",
     "iopub.status.idle": "2024-02-28T11:53:53.249365Z",
     "shell.execute_reply": "2024-02-28T11:53:53.248619Z",
     "shell.execute_reply.started": "2024-02-28T11:53:53.244273Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'standard_input_data_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/workspace/Baseline/tranditional_ml.ipynb Cell 31\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/workspace/Baseline/tranditional_ml.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mlen\u001b[39m(standard_input_data_list), \u001b[39mlen\u001b[39m(standard_input_data_list[\u001b[39m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'standard_input_data_list' is not defined"
     ]
    }
   ],
   "source": [
    "len(standard_input_data_list), len(standard_input_data_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:53:53.250856Z",
     "iopub.status.busy": "2024-02-28T11:53:53.250360Z",
     "iopub.status.idle": "2024-02-28T11:53:53.290521Z",
     "shell.execute_reply": "2024-02-28T11:53:53.289838Z",
     "shell.execute_reply.started": "2024-02-28T11:53:53.250826Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'standard_input_data_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/workspace/Baseline/tranditional_ml.ipynb Cell 32\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/workspace/Baseline/tranditional_ml.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m dataset_np \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(standard_input_data_list)\n\u001b[1;32m      <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/workspace/Baseline/tranditional_ml.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m dataset_np \u001b[39m=\u001b[39m dataset_np\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m14\u001b[39m, \u001b[39m500\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/workspace/Baseline/tranditional_ml.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m dataset_np\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'standard_input_data_list' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_np = np.array(standard_input_data_list)\n",
    "dataset_np = dataset_np.reshape(-1, 14, 500)\n",
    "dataset_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:53:53.292156Z",
     "iopub.status.busy": "2024-02-28T11:53:53.291626Z",
     "iopub.status.idle": "2024-02-28T11:53:53.299358Z",
     "shell.execute_reply": "2024-02-28T11:53:53.298719Z",
     "shell.execute_reply.started": "2024-02-28T11:53:53.292125Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_list = list(dataset_np)\n",
    "label_list = [0] * 300 + [1] * 300 + [2] * 300 + [3] * 300\n",
    "data_df = pd.DataFrame({'raw data': dataset_list, 'label': label_list})\n",
    "x = deepcopy(data_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({'raw data': [test], 'label': [0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:53:53.300886Z",
     "iopub.status.busy": "2024-02-28T11:53:53.300410Z",
     "iopub.status.idle": "2024-02-28T11:53:53.305819Z",
     "shell.execute_reply": "2024-02-28T11:53:53.305113Z",
     "shell.execute_reply.started": "2024-02-28T11:53:53.300857Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 计算过零率\n",
    "\n",
    "def zero_crossing_rate(trials):\n",
    "    '''\n",
    "    计算一个trials(二维列表)各个通道的过零率, 返回一个list\n",
    "    '''\n",
    "\n",
    "    def compute(signal):\n",
    "        '''\n",
    "        计算一维信号的过零率\n",
    "        '''\n",
    "        crossings = np.where(np.diff(np.sign(signal)))[0]\n",
    "        zero_crossing_rate = len(crossings) / len(signal)\n",
    "        return zero_crossing_rate\n",
    "\n",
    "    ret = [compute(trials[i]) for i in range(len(trials))]\n",
    "    return np.array(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:53:53.307098Z",
     "iopub.status.busy": "2024-02-28T11:53:53.306714Z",
     "iopub.status.idle": "2024-02-28T11:53:53.316353Z",
     "shell.execute_reply": "2024-02-28T11:53:53.315735Z",
     "shell.execute_reply.started": "2024-02-28T11:53:53.307069Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.168, 0.04 , 0.13 , 0.102, 0.198, 0.15 , 0.14 , 0.186, 0.126,\n",
       "        0.074, 0.106, 0.114, 0.062, 0.046]),\n",
       " array([0.15 , 0.088, 0.156, 0.084, 0.108, 0.112, 0.174, 0.184, 0.18 ,\n",
       "        0.136, 0.108, 0.184, 0.08 , 0.104]),\n",
       " array([0.138, 0.1  , 0.078, 0.07 , 0.05 , 0.154, 0.184, 0.09 , 0.172,\n",
       "        0.094, 0.12 , 0.124, 0.062, 0.112]))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 同一类别的过零率差别大\n",
    "np.array(zero_crossing_rate(x['raw data'])),\\\n",
    "     np.array(zero_crossing_rate(data_df.iloc[1]['raw data'])),\\\n",
    "       np.array(zero_crossing_rate(data_df.iloc[2]['raw data'])  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:53:53.317788Z",
     "iopub.status.busy": "2024-02-28T11:53:53.317315Z",
     "iopub.status.idle": "2024-02-28T11:53:53.325391Z",
     "shell.execute_reply": "2024-02-28T11:53:53.324764Z",
     "shell.execute_reply.started": "2024-02-28T11:53:53.317758Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.056, 0.084, 0.112, 0.106, 0.05 , 0.066, 0.062, 0.17 , 0.136,\n",
       "        0.076, 0.128, 0.138, 0.1  , 0.084]),\n",
       " array([0.106, 0.056, 0.098, 0.054, 0.028, 0.04 , 0.06 , 0.104, 0.076,\n",
       "        0.064, 0.032, 0.068, 0.046, 0.076]),\n",
       " array([0.012, 0.008, 0.016, 0.054, 0.026, 0.014, 0.018, 0.058, 0.05 ,\n",
       "        0.042, 0.054, 0.032, 0.034, 0.048]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 同一类别的过零率差别大\n",
    "np.array(zero_crossing_rate(data_df.iloc[398]['raw data'])),\\\n",
    "     np.array(zero_crossing_rate(data_df.iloc[399]['raw data'])),\\\n",
    "       np.array(zero_crossing_rate(data_df.iloc[400]['raw data'])  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:53:53.326562Z",
     "iopub.status.busy": "2024-02-28T11:53:53.326291Z",
     "iopub.status.idle": "2024-02-28T11:53:53.330570Z",
     "shell.execute_reply": "2024-02-28T11:53:53.329872Z",
     "shell.execute_reply.started": "2024-02-28T11:53:53.326536Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 计算均值\n",
    "\n",
    "def calculate_channel_means(eeg_data):\n",
    "    \"\"\"\n",
    "    计算每个通道的均值。\n",
    "\n",
    "    参数：\n",
    "    - eeg_data: 二维 NumPy 数组，表示 EEG 数据，形状为 (n_channels, n_points)。\n",
    "\n",
    "    返回：\n",
    "    - channel_means: 一维 NumPy 数组，包含每个通道的均值。\n",
    "    \"\"\"\n",
    "    # 计算每个通道的均值，axis=2 表示在样本点上求均值\n",
    "    channel_means = np.mean(eeg_data, axis=1)\n",
    "\n",
    "    return channel_means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:53:53.331975Z",
     "iopub.status.busy": "2024-02-28T11:53:53.331527Z",
     "iopub.status.idle": "2024-02-28T11:53:53.337985Z",
     "shell.execute_reply": "2024-02-28T11:53:53.337333Z",
     "shell.execute_reply.started": "2024-02-28T11:53:53.331945Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  5.79579612,  -6.37368547,  -1.11140977, -10.55618282,\n",
       "          2.01619029,  -1.79062357,  -1.40401936,   0.80076936,\n",
       "          3.03901338,   4.1266799 ,  11.40424161,  -0.07762688,\n",
       "         26.9461035 ,  -1.1365759 ]),\n",
       " array([ -7.05026265, -18.51313891,  -5.52623322,  -6.18110454,\n",
       "          5.75653787,  10.04083053,  -0.65157368,   0.92565608,\n",
       "         -1.22593159,  -4.27781215,   6.5364136 ,   3.26128473,\n",
       "          6.60940732,   5.42792899]))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 同一类别的trials均值差别怎么这么大？\n",
    "calculate_channel_means(data_df.iloc[0]['raw data']), calculate_channel_means(data_df.iloc[1]['raw data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:53:53.339377Z",
     "iopub.status.busy": "2024-02-28T11:53:53.338928Z",
     "iopub.status.idle": "2024-02-28T11:53:53.343111Z",
     "shell.execute_reply": "2024-02-28T11:53:53.342449Z",
     "shell.execute_reply.started": "2024-02-28T11:53:53.339348Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 计算标准差\n",
    "\n",
    "def calculate_channel_stds(eeg_data):\n",
    "    \"\"\"\n",
    "    计算每个通道的均值。\n",
    "\n",
    "    参数：\n",
    "    - eeg_data: 二维 NumPy 数组，表示 EEG 数据，形状为 (n_channels, n_points)。\n",
    "\n",
    "    返回：\n",
    "    - channel_means: 一维 NumPy 数组，包含每个通道的均值。\n",
    "    \"\"\"\n",
    "    # 计算每个通道的均值，axis=2 表示在样本点上求均值\n",
    "    channel_stds = np.std(eeg_data, axis=1)\n",
    "\n",
    "    return channel_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:53:53.344711Z",
     "iopub.status.busy": "2024-02-28T11:53:53.344171Z",
     "iopub.status.idle": "2024-02-28T11:53:53.349736Z",
     "shell.execute_reply": "2024-02-28T11:53:53.349019Z",
     "shell.execute_reply.started": "2024-02-28T11:53:53.344656Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_first_order_diff(eeg_data):\n",
    "\n",
    "    first_order_diff = np.sum(np.abs(np.diff(eeg_data, axis=1)), axis=1) / (eeg_data.shape[1] - 1)\n",
    "\n",
    "    return first_order_diff\n",
    "\n",
    "def calculate_second_order_diff(eeg_data):\n",
    "    \n",
    "    second_order_diff = np.sum(np.abs(np.diff(eeg_data, n=2, axis=1)), axis=1) / (eeg_data.shape[1] - 2)\n",
    "\n",
    "    return second_order_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:53:53.351209Z",
     "iopub.status.busy": "2024-02-28T11:53:53.350773Z",
     "iopub.status.idle": "2024-02-28T11:53:53.359403Z",
     "shell.execute_reply": "2024-02-28T11:53:53.358680Z",
     "shell.execute_reply.started": "2024-02-28T11:53:53.351181Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def time_domain_feature(eeg_dataset):\n",
    "    '''\n",
    "    提取时域特征\n",
    "    params:\n",
    "    - eeg_dataset: Dataframe类型\n",
    "    '''\n",
    "    zero_crossing_list = []\n",
    "    means_list = []\n",
    "    stds_list = []\n",
    "    first_order_diff_list = []\n",
    "    second_order_diff_list = []\n",
    "\n",
    "    for _, row in eeg_dataset.iterrows():\n",
    "        zero_crossing = zero_crossing_rate(row['raw data'])\n",
    "        zero_crossing_list.append(zero_crossing)\n",
    "        means = calculate_channel_means(row['raw data'])\n",
    "        means_list.append(means)\n",
    "        stds = calculate_channel_stds(row['raw data'])\n",
    "        stds_list.append(stds)\n",
    "        first_diff = calculate_first_order_diff(row['raw data'])\n",
    "        first_order_diff_list.append(first_diff)\n",
    "        second_diff = calculate_second_order_diff(row['raw data'])\n",
    "        second_order_diff_list.append(second_diff)\n",
    "    \n",
    "    nChannels = zero_crossing_list[0].shape[0]\n",
    "    for i in range(nChannels):\n",
    "        eeg_dataset[f'zero_crossing_rate_c{i+1}'] = np.array(zero_crossing_list)[:,i]\n",
    "        eeg_dataset[f'mean_c{i+1}'] = np.array(means_list)[:,i]\n",
    "        eeg_dataset[f'std_c{i+1}'] = np.array(stds_list)[:,i]\n",
    "        eeg_dataset[f'first_order_diff_c{i+1}'] = np.array(first_order_diff_list)[:,i]\n",
    "        eeg_dataset[f'second_order_diff_c{i+1}'] = np.array(second_order_diff_list)[:,i]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:53:53.360804Z",
     "iopub.status.busy": "2024-02-28T11:53:53.360341Z",
     "iopub.status.idle": "2024-02-28T11:53:53.882533Z",
     "shell.execute_reply": "2024-02-28T11:53:53.881943Z",
     "shell.execute_reply.started": "2024-02-28T11:53:53.360776Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_domain_feature(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:53:53.884193Z",
     "iopub.status.busy": "2024-02-28T11:53:53.883628Z",
     "iopub.status.idle": "2024-02-28T11:53:53.907829Z",
     "shell.execute_reply": "2024-02-28T11:53:53.907254Z",
     "shell.execute_reply.started": "2024-02-28T11:53:53.884169Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1200 entries, 0 to 1199\n",
      "Data columns (total 72 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   raw data                1200 non-null   object \n",
      " 1   label                   1200 non-null   int64  \n",
      " 2   zero_crossing_rate_c1   1200 non-null   float64\n",
      " 3   mean_c1                 1200 non-null   float64\n",
      " 4   std_c1                  1200 non-null   float64\n",
      " 5   first_order_diff_c1     1200 non-null   float64\n",
      " 6   second_order_diff_c1    1200 non-null   float64\n",
      " 7   zero_crossing_rate_c2   1200 non-null   float64\n",
      " 8   mean_c2                 1200 non-null   float64\n",
      " 9   std_c2                  1200 non-null   float64\n",
      " 10  first_order_diff_c2     1200 non-null   float64\n",
      " 11  second_order_diff_c2    1200 non-null   float64\n",
      " 12  zero_crossing_rate_c3   1200 non-null   float64\n",
      " 13  mean_c3                 1200 non-null   float64\n",
      " 14  std_c3                  1200 non-null   float64\n",
      " 15  first_order_diff_c3     1200 non-null   float64\n",
      " 16  second_order_diff_c3    1200 non-null   float64\n",
      " 17  zero_crossing_rate_c4   1200 non-null   float64\n",
      " 18  mean_c4                 1200 non-null   float64\n",
      " 19  std_c4                  1200 non-null   float64\n",
      " 20  first_order_diff_c4     1200 non-null   float64\n",
      " 21  second_order_diff_c4    1200 non-null   float64\n",
      " 22  zero_crossing_rate_c5   1200 non-null   float64\n",
      " 23  mean_c5                 1200 non-null   float64\n",
      " 24  std_c5                  1200 non-null   float64\n",
      " 25  first_order_diff_c5     1200 non-null   float64\n",
      " 26  second_order_diff_c5    1200 non-null   float64\n",
      " 27  zero_crossing_rate_c6   1200 non-null   float64\n",
      " 28  mean_c6                 1200 non-null   float64\n",
      " 29  std_c6                  1200 non-null   float64\n",
      " 30  first_order_diff_c6     1200 non-null   float64\n",
      " 31  second_order_diff_c6    1200 non-null   float64\n",
      " 32  zero_crossing_rate_c7   1200 non-null   float64\n",
      " 33  mean_c7                 1200 non-null   float64\n",
      " 34  std_c7                  1200 non-null   float64\n",
      " 35  first_order_diff_c7     1200 non-null   float64\n",
      " 36  second_order_diff_c7    1200 non-null   float64\n",
      " 37  zero_crossing_rate_c8   1200 non-null   float64\n",
      " 38  mean_c8                 1200 non-null   float64\n",
      " 39  std_c8                  1200 non-null   float64\n",
      " 40  first_order_diff_c8     1200 non-null   float64\n",
      " 41  second_order_diff_c8    1200 non-null   float64\n",
      " 42  zero_crossing_rate_c9   1200 non-null   float64\n",
      " 43  mean_c9                 1200 non-null   float64\n",
      " 44  std_c9                  1200 non-null   float64\n",
      " 45  first_order_diff_c9     1200 non-null   float64\n",
      " 46  second_order_diff_c9    1200 non-null   float64\n",
      " 47  zero_crossing_rate_c10  1200 non-null   float64\n",
      " 48  mean_c10                1200 non-null   float64\n",
      " 49  std_c10                 1200 non-null   float64\n",
      " 50  first_order_diff_c10    1200 non-null   float64\n",
      " 51  second_order_diff_c10   1200 non-null   float64\n",
      " 52  zero_crossing_rate_c11  1200 non-null   float64\n",
      " 53  mean_c11                1200 non-null   float64\n",
      " 54  std_c11                 1200 non-null   float64\n",
      " 55  first_order_diff_c11    1200 non-null   float64\n",
      " 56  second_order_diff_c11   1200 non-null   float64\n",
      " 57  zero_crossing_rate_c12  1200 non-null   float64\n",
      " 58  mean_c12                1200 non-null   float64\n",
      " 59  std_c12                 1200 non-null   float64\n",
      " 60  first_order_diff_c12    1200 non-null   float64\n",
      " 61  second_order_diff_c12   1200 non-null   float64\n",
      " 62  zero_crossing_rate_c13  1200 non-null   float64\n",
      " 63  mean_c13                1200 non-null   float64\n",
      " 64  std_c13                 1200 non-null   float64\n",
      " 65  first_order_diff_c13    1200 non-null   float64\n",
      " 66  second_order_diff_c13   1200 non-null   float64\n",
      " 67  zero_crossing_rate_c14  1200 non-null   float64\n",
      " 68  mean_c14                1200 non-null   float64\n",
      " 69  std_c14                 1200 non-null   float64\n",
      " 70  first_order_diff_c14    1200 non-null   float64\n",
      " 71  second_order_diff_c14   1200 non-null   float64\n",
      "dtypes: float64(70), int64(1), object(1)\n",
      "memory usage: 675.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 频域特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:53:53.908972Z",
     "iopub.status.busy": "2024-02-28T11:53:53.908576Z",
     "iopub.status.idle": "2024-02-28T11:53:53.915305Z",
     "shell.execute_reply": "2024-02-28T11:53:53.914770Z",
     "shell.execute_reply.started": "2024-02-28T11:53:53.908954Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.signal import welch\n",
    "\n",
    "def five_band_energy(eeg_data, fs=125):\n",
    "    \n",
    "    energy = []\n",
    "    \n",
    "    for eeg_signal in eeg_data:\n",
    "        # 计算功率谱密度（PSD）\n",
    "        frequencies, psd = welch(eeg_signal, fs, nperseg=1024)\n",
    "\n",
    "        # 定义频带边界\n",
    "        delta_band = (0.5, 4)\n",
    "        theta_band = (4, 8)\n",
    "        alpha_band = (8, 14)\n",
    "        beta_band = (14, 30)\n",
    "        gamma_band = (30, 60)\n",
    "\n",
    "        # 计算每个频带内的能量\n",
    "        energy.append(np.trapz(psd[(frequencies >= delta_band[0]) & (frequencies <= delta_band[1])], \\\n",
    "            frequencies[(frequencies >= delta_band[0]) & (frequencies <= delta_band[1])]))\n",
    "        energy.append(np.trapz(psd[(frequencies >= theta_band[0]) & (frequencies <= theta_band[1])], \\\n",
    "            frequencies[(frequencies >= theta_band[0]) & (frequencies <= theta_band[1])]))\n",
    "        energy.append(np.trapz(psd[(frequencies >= alpha_band[0]) & (frequencies <= alpha_band[1])], \\\n",
    "            frequencies[(frequencies >= alpha_band[0]) & (frequencies <= alpha_band[1])]))\n",
    "        energy.append(np.trapz(psd[(frequencies >= beta_band[0]) & (frequencies <= beta_band[1])], \\\n",
    "            frequencies[(frequencies >= beta_band[0]) & (frequencies <= beta_band[1])]))\n",
    "        energy.append(np.trapz(psd[(frequencies >= gamma_band[0]) & (frequencies <= gamma_band[1])], \\\n",
    "            frequencies[(frequencies >= gamma_band[0]) & (frequencies <= gamma_band[1])]))\n",
    "\n",
    "    return np.array(energy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:53:53.916140Z",
     "iopub.status.busy": "2024-02-28T11:53:53.915961Z",
     "iopub.status.idle": "2024-02-28T11:53:53.920326Z",
     "shell.execute_reply": "2024-02-28T11:53:53.919800Z",
     "shell.execute_reply.started": "2024-02-28T11:53:53.916124Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def freq_domain_feature(eeg_dataset):\n",
    "    '''\n",
    "    提取时域特征\n",
    "\n",
    "    参数:\n",
    "    - eeg_dataset: Dataframe类型\n",
    "    '''\n",
    "    \n",
    "    energy = []\n",
    "\n",
    "    for _, row in eeg_dataset.iterrows():\n",
    "        \n",
    "        energy.append(five_band_energy(row['raw data']))\n",
    "\n",
    "    freq_names = ['delta', 'theta', 'alpha', 'beta', 'gamma']\n",
    "    nFreqs = len(freq_names)\n",
    "    nChannels = energy[0].shape[0] // nFreqs\n",
    "    \n",
    "    for i in range(nChannels):\n",
    "        for j in range(nFreqs):\n",
    "            eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:53:53.921184Z",
     "iopub.status.busy": "2024-02-28T11:53:53.921007Z",
     "iopub.status.idle": "2024-02-28T11:53:59.421482Z",
     "shell.execute_reply": "2024-02-28T11:53:59.420804Z",
     "shell.execute_reply.started": "2024-02-28T11:53:53.921167Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pai/lib/python3.11/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 1024 is greater than input length  = 500, using nperseg = 500\n",
      "  warnings.warn('nperseg = {0:d} is greater than input length '\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_390/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n"
     ]
    }
   ],
   "source": [
    "freq_domain_feature(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pai/lib/python3.11/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 1024 is greater than input length  = 500, using nperseg = 500\n",
      "  warnings.warn('nperseg = {0:d} is greater than input length '\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_26126/1711663099.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw data</th>\n",
       "      <th>label</th>\n",
       "      <th>zero_crossing_rate_c1</th>\n",
       "      <th>mean_c1</th>\n",
       "      <th>std_c1</th>\n",
       "      <th>first_order_diff_c1</th>\n",
       "      <th>second_order_diff_c1</th>\n",
       "      <th>zero_crossing_rate_c2</th>\n",
       "      <th>mean_c2</th>\n",
       "      <th>std_c2</th>\n",
       "      <th>...</th>\n",
       "      <th>delta_c13</th>\n",
       "      <th>theta_c13</th>\n",
       "      <th>alpha_c13</th>\n",
       "      <th>beta_c13</th>\n",
       "      <th>gamma_c13</th>\n",
       "      <th>delta_c14</th>\n",
       "      <th>theta_c14</th>\n",
       "      <th>alpha_c14</th>\n",
       "      <th>beta_c14</th>\n",
       "      <th>gamma_c14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[1.2233674002786974, 5.311362745323572, 9.384...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.168</td>\n",
       "      <td>5.795796</td>\n",
       "      <td>26.747689</td>\n",
       "      <td>6.917654</td>\n",
       "      <td>7.757805</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-6.373685</td>\n",
       "      <td>40.206396</td>\n",
       "      <td>...</td>\n",
       "      <td>232.707939</td>\n",
       "      <td>30.929507</td>\n",
       "      <td>19.584694</td>\n",
       "      <td>27.794255</td>\n",
       "      <td>10.281614</td>\n",
       "      <td>380.16133</td>\n",
       "      <td>25.192386</td>\n",
       "      <td>8.704919</td>\n",
       "      <td>11.973297</td>\n",
       "      <td>4.960753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 142 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            raw data  label  \\\n",
       "0  [[1.2233674002786974, 5.311362745323572, 9.384...      0   \n",
       "\n",
       "   zero_crossing_rate_c1   mean_c1     std_c1  first_order_diff_c1  \\\n",
       "0                  0.168  5.795796  26.747689             6.917654   \n",
       "\n",
       "   second_order_diff_c1  zero_crossing_rate_c2   mean_c2     std_c2  ...  \\\n",
       "0              7.757805                   0.04 -6.373685  40.206396  ...   \n",
       "\n",
       "    delta_c13  theta_c13  alpha_c13   beta_c13  gamma_c13  delta_c14  \\\n",
       "0  232.707939  30.929507  19.584694  27.794255  10.281614  380.16133   \n",
       "\n",
       "   theta_c14  alpha_c14   beta_c14  gamma_c14  \n",
       "0  25.192386   8.704919  11.973297   4.960753  \n",
       "\n",
       "[1 rows x 142 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_domain_feature(test_df)\n",
    "freq_domain_feature(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [[1.2233674002786974, 5.311362745323572, 9.384...\n",
       "Name: raw data, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['raw data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:53:59.422821Z",
     "iopub.status.busy": "2024-02-28T11:53:59.422321Z",
     "iopub.status.idle": "2024-02-28T11:53:59.431219Z",
     "shell.execute_reply": "2024-02-28T11:53:59.430643Z",
     "shell.execute_reply.started": "2024-02-28T11:53:59.422799Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1200 entries, 0 to 1199\n",
      "Columns: 142 entries, raw data to gamma_c14\n",
      "dtypes: float64(140), int64(1), object(1)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-02-28T11:53:59.432272Z",
     "iopub.status.busy": "2024-02-28T11:53:59.431967Z",
     "iopub.status.idle": "2024-02-28T11:53:59.994863Z",
     "shell.execute_reply": "2024-02-28T11:53:59.994225Z",
     "shell.execute_reply.started": "2024-02-28T11:53:59.432254Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_path = os.getcwd() + \"/lyh_data/session1/Feature_extracted\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "    \n",
    "save_path += \"/lyh_feature_extracted.csv\"\n",
    "    \n",
    "data_df.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:53:59.996024Z",
     "iopub.status.busy": "2024-02-28T11:53:59.995741Z",
     "iopub.status.idle": "2024-02-28T11:54:00.178541Z",
     "shell.execute_reply": "2024-02-28T11:54:00.177942Z",
     "shell.execute_reply.started": "2024-02-28T11:53:59.996002Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 构造数据集\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 打乱数据集\n",
    "shuffled_df = data_df.sample(frac=1, random_state=42)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "train_df, test_df = train_test_split(shuffled_df, test_size=0.2, random_state=42)\n",
    "y_train = train_df['label']\n",
    "X_train = train_df.drop(columns=['label', 'raw data'])\n",
    "y_test = test_df['label']\n",
    "X_test = test_df.drop(columns=['label', 'raw data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不存在相等的元素\n"
     ]
    }
   ],
   "source": [
    "train_id = train_df.index.to_list()\n",
    "test_id = test_df.index.to_list()\n",
    "for item in train_id:\n",
    "    if item in test_id:\n",
    "        print(\"存在相等的元素\")\n",
    "        break\n",
    "else:\n",
    "    print(\"不存在相等的元素\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:54:00.179805Z",
     "iopub.status.busy": "2024-02-28T11:54:00.179403Z",
     "iopub.status.idle": "2024-02-28T11:54:00.183962Z",
     "shell.execute_reply": "2024-02-28T11:54:00.183395Z",
     "shell.execute_reply.started": "2024-02-28T11:54:00.179783Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((960, 140), (960,), (240, 140), (240,))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:54:01.792886Z",
     "iopub.status.busy": "2024-02-28T11:54:01.792607Z",
     "iopub.status.idle": "2024-02-28T11:54:03.046647Z",
     "shell.execute_reply": "2024-02-28T11:54:03.045839Z",
     "shell.execute_reply.started": "2024-02-28T11:54:01.792861Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002070 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33576\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 140\n",
      "[LightGBM] [Info] Start training from score -1.382136\n",
      "[LightGBM] [Info] Start training from score -1.365675\n",
      "[LightGBM] [Info] Start training from score -1.361602\n",
      "[LightGBM] [Info] Start training from score -1.437588\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier()"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgm = LGBMClassifier()\n",
    "lgm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:54:03.053790Z",
     "iopub.status.busy": "2024-02-28T11:54:03.053123Z",
     "iopub.status.idle": "2024-02-28T11:54:03.080211Z",
     "shell.execute_reply": "2024-02-28T11:54:03.079475Z",
     "shell.execute_reply.started": "2024-02-28T11:54:03.053766Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.9125)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 手工特征提取+未调参LightGBM\n",
    "y_train_pred = lgm.predict(X_train)\n",
    "acc_train = (y_train == y_train_pred).sum() / len(y_train)\n",
    "\n",
    "y_test_pred = lgm.predict(X_test)\n",
    "acc_test = (y_test == y_test_pred).sum() / len(y_test)\n",
    "acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix:\n",
      " [[56  1  0  2]\n",
      " [ 6 46  1  2]\n",
      " [ 0  0 54  0]\n",
      " [ 6  2  1 63]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"confusion_matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:54:03.081841Z",
     "iopub.status.busy": "2024-02-28T11:54:03.081308Z",
     "iopub.status.idle": "2024-02-28T11:56:07.407255Z",
     "shell.execute_reply": "2024-02-28T11:56:07.406487Z",
     "shell.execute_reply.started": "2024-02-28T11:54:03.081810Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.1359676   -3.56294757  -4.93683818 ... -24.83999672 -20.72275314\n",
      "  -18.75059175]\n",
      " [ -1.57088852 -10.06453023 -10.46421132 ... -38.55049664 -27.24653005\n",
      "  -23.78624649]\n",
      " [  0.55504042  -7.72564738  -4.94103614 ... -20.07059185 -20.46490629\n",
      "  -13.69915833]\n",
      " ...\n",
      " [ -4.28028626 -10.41083709  -7.68718556 ... -13.32182841 -16.75106415\n",
      "  -10.86976717]\n",
      " [  6.45557612   9.23443594  15.71743304 ... -55.61471825 -51.56785603\n",
      "  -36.44763422]\n",
      " [ -2.82070615  -1.63619278   1.08530902 ...  -1.0391331   -4.03671668\n",
      "   -0.33247381]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 5.1359676 , -3.56294757, -4.93683818, ..., -1.0391331 ,\n",
       "       -4.03671668, -0.33247381])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 原始数据进行训练\n",
    "shuffled_df = data_df.sample(frac=1, random_state=42)\n",
    "train_df, test_df = train_test_split(shuffled_df, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = np.array(train_df['raw data'])\n",
    "print(X_train[0])\n",
    "X_test = np.array(test_df['raw data'])\n",
    "X_train = np.vstack(X_train).reshape(-1, 14, 500)\n",
    "X_test = np.vstack(X_test).reshape(-1, 14, 500)\n",
    "X_train = X_train.reshape(-1, 14 * 500)\n",
    "X_test = X_test.reshape(-1, 14 * 500)\n",
    "y_train = np.array(train_df['label'])\n",
    "y_test = np.array(test_df['label'])\n",
    "X_train[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.270247 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1785000\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 7000\n",
      "[LightGBM] [Info] Start training from score -1.382136\n",
      "[LightGBM] [Info] Start training from score -1.365675\n",
      "[LightGBM] [Info] Start training from score -1.361602\n",
      "[LightGBM] [Info] Start training from score -1.437588\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0, 0.4875)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgm = LGBMClassifier()\n",
    "lgm.fit(X_train, y_train)\n",
    "\n",
    "# 原始特征+未调参LightGBM\n",
    "y_train_pred = lgm.predict(X_train)\n",
    "acc_train = (y_train == y_train_pred).sum() / len(y_train)\n",
    "\n",
    "y_test_pred = lgm.predict(X_test)\n",
    "acc_test = (y_test == y_test_pred).sum() / len(y_test)\n",
    "acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:56:07.409079Z",
     "iopub.status.busy": "2024-02-28T11:56:07.408389Z",
     "iopub.status.idle": "2024-02-28T11:57:35.972249Z",
     "shell.execute_reply": "2024-02-28T11:57:35.971589Z",
     "shell.execute_reply.started": "2024-02-28T11:56:07.409046Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.48333333333333334)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "# 原始数据进行训练\n",
    "\n",
    "shuffled_df = data_df.sample(frac=1, random_state=42)\n",
    "train_df, test_df = train_test_split(shuffled_df, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = np.array(train_df['raw data'])\n",
    "X_test = np.array(test_df['raw data'])\n",
    "X_train = np.vstack(X_train).reshape(-1, 14, 500)\n",
    "X_test = np.vstack(X_test).reshape(-1, 14, 500)\n",
    "X_train = X_train.reshape(-1, 14 * 500)\n",
    "X_test = X_test.reshape(-1, 14 * 500)\n",
    "y_train = np.array(train_df['label'])\n",
    "y_test = np.array(test_df['label'])\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# 原始特征+未调参xgboost\n",
    "y_train_pred = xgb.predict(X_train)\n",
    "acc_train = (y_train == y_train_pred).sum() / len(y_train)\n",
    "\n",
    "y_test_pred = xgb.predict(X_test)\n",
    "acc_test = (y_test == y_test_pred).sum() / len(y_test)\n",
    "acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:57:35.973496Z",
     "iopub.status.busy": "2024-02-28T11:57:35.973129Z",
     "iopub.status.idle": "2024-02-28T11:57:42.619119Z",
     "shell.execute_reply": "2024-02-28T11:57:42.618449Z",
     "shell.execute_reply.started": "2024-02-28T11:57:35.973474Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.925)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构造数据集\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 打乱数据集\n",
    "shuffled_df = data_df.sample(frac=1, random_state=42)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "train_df, test_df = train_test_split(shuffled_df, test_size=0.2, random_state=42)\n",
    "y_train = train_df['label']\n",
    "X_train = train_df.drop(columns=['label', 'raw data'])\n",
    "y_test = test_df['label']\n",
    "X_test = test_df.drop(columns=['label', 'raw data'])\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train, y_train)\n",
    "# xgb.save_model(\"xgb.json\")\n",
    "\n",
    "# 手工特征提取+未调参XGBoost\n",
    "y_train_pred = xgb.predict(X_train)\n",
    "# acc_train = (y_train == y_train_pred).sum() / len(y_train)\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "y_test_pred = xgb.predict(X_test)\n",
    "# acc_test = (y_test == y_test_pred).sum() / len(y_test)\n",
    "acc_test = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier()\n",
    "clf.load_model('./xgb_4s_se1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([np.ones(shape=(140,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.925)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred = clf.predict(X_train)\n",
    "# acc_train = (y_train == y_train_pred).sum() / len(y_train)\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "y_test_pred = clf.predict(X_test)\n",
    "# acc_test = (y_test == y_test_pred).sum() / len(y_test)\n",
    "acc_test = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2560 entries, 1172 to 1476\n",
      "Columns: 140 entries, zero_crossing_rate_c1 to gamma_c14\n",
      "dtypes: float64(140)\n",
      "memory usage: 2.8 MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:57:42.620230Z",
     "iopub.status.busy": "2024-02-28T11:57:42.619953Z",
     "iopub.status.idle": "2024-02-28T11:57:42.624374Z",
     "shell.execute_reply": "2024-02-28T11:57:42.623728Z",
     "shell.execute_reply.started": "2024-02-28T11:57:42.620210Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Parameters:\n",
      "objective: multi:softprob\n",
      "base_score: None\n",
      "booster: None\n",
      "callbacks: None\n",
      "colsample_bylevel: None\n",
      "colsample_bynode: None\n",
      "colsample_bytree: None\n",
      "device: None\n",
      "early_stopping_rounds: None\n",
      "enable_categorical: False\n",
      "eval_metric: None\n",
      "feature_types: None\n",
      "gamma: None\n",
      "grow_policy: None\n",
      "importance_type: None\n",
      "interaction_constraints: None\n",
      "learning_rate: None\n",
      "max_bin: None\n",
      "max_cat_threshold: None\n",
      "max_cat_to_onehot: None\n",
      "max_delta_step: None\n",
      "max_depth: None\n",
      "max_leaves: None\n",
      "min_child_weight: None\n",
      "missing: nan\n",
      "monotone_constraints: None\n",
      "multi_strategy: None\n",
      "n_estimators: None\n",
      "n_jobs: None\n",
      "num_parallel_tree: None\n",
      "random_state: None\n",
      "reg_alpha: None\n",
      "reg_lambda: None\n",
      "sampling_method: None\n",
      "scale_pos_weight: None\n",
      "subsample: None\n",
      "tree_method: None\n",
      "validate_parameters: None\n",
      "verbosity: None\n"
     ]
    }
   ],
   "source": [
    "default_params = xgb.get_params()\n",
    "# 打印参数\n",
    "print(\"Default Parameters:\")\n",
    "for param, value in default_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:57:42.625593Z",
     "iopub.status.busy": "2024-02-28T11:57:42.625219Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 1. 确定learning_rate和n_estimators\n",
    "param_test1 = {\n",
    " 'learning_rate': [0.2, 0.25 ,0.300000012, 0.35],\n",
    " 'n_estimators': [90, 95, 100, 105]\n",
    "}\n",
    "\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier(), \\\n",
    "    param_grid = param_test1, scoring='accuracy', cv=3)\n",
    "gsearch1.fit(pd.concat([X_train, X_test],axis=0), pd.concat([y_train,y_test],axis=0))\n",
    "print(f\"best params: {gsearch1.best_params_}\\nbest score:  {gsearch1.best_score_}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'learning_rate': 0.35,\n",
    "    'n_estimators': 90\n",
    "}\n",
    "\n",
    "model1 = XGBClassifier(**params)\n",
    "model1.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = model1.predict(X_train)\n",
    "acc_train = (y_train == y_train_pred).sum() / len(y_train)\n",
    "\n",
    "y_test_pred = model1.predict(X_test)\n",
    "acc_test = (y_test == y_test_pred).sum() / len(y_test)\n",
    "\n",
    "acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 2. 确定max_depth和min_child_weight\n",
    "params = {\n",
    "    'learning_rate': 0.35,\n",
    "    'n_estimators': 90\n",
    "}\n",
    "\n",
    "param_test2 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}\n",
    "\n",
    "gsearch2 = GridSearchCV(estimator = XGBClassifier(**param), \\\n",
    "    param_grid = param_test2, scoring='accuracy', cv=3)\n",
    "gsearch2.fit(pd.concat([X_train, X_test],axis=0), pd.concat([y_train,y_test],axis=0))\n",
    "print(f\"best params: {gsearch2.best_params_}\\nbest score:  {gsearch2.best_score_}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'learning_rate': 0.35,\n",
    "    'n_estimators': 90,\n",
    "    'max_depth': 9, \n",
    "    'min_child_weight': 5\n",
    "}\n",
    "\n",
    "model2 = XGBClassifier(**params)\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = model2.predict(X_train)\n",
    "acc_train = (y_train == y_train_pred).sum() / len(y_train)\n",
    "\n",
    "y_test_pred = model2.predict(X_test)\n",
    "acc_test = (y_test == y_test_pred).sum() / len(y_test)\n",
    "\n",
    "acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 3. 确定gamma\n",
    "params = {\n",
    "    'learning_rate': 0.35,\n",
    "    'n_estimators': 90,\n",
    "    'max_depth': 9, \n",
    "    'min_child_weight': 5\n",
    "}\n",
    "\n",
    "param_test3 = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "\n",
    "gsearch3 = GridSearchCV(estimator = XGBClassifier(**param), \\\n",
    "    param_grid = param_test3, scoring='accuracy', cv=3)\n",
    "gsearch3.fit(pd.concat([X_train, X_test],axis=0), pd.concat([y_train,y_test],axis=0))\n",
    "print(f\"best params: {gsearch3.best_params_}\\nbest score:  {gsearch3.best_score_}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 4. 确定reg_alpha\n",
    "param = {\n",
    "    'learning_rate': 0.3,\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 1,\n",
    "    'gamma': 0.0\n",
    "}\n",
    "\n",
    "param_test4 = {\n",
    " 'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05]\n",
    "}\n",
    "\n",
    "gsearch4 = GridSearchCV(estimator = XGBClassifier(**param), \\\n",
    "    param_grid = param_test4, scoring='accuracy', cv=3)\n",
    "gsearch4.fit(pd.concat([X_train, X_test],axis=0), pd.concat([y_train,y_test],axis=0))\n",
    "print(f\"best params: {gsearch4.best_params_}\\nbest score:  {gsearch4.best_score_}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'learning_rate': 0.3,\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 9, \n",
    "    'min_child_weight': 5,\n",
    "    'reg_alpha': 0.01\n",
    "}\n",
    "\n",
    "model4 = XGBClassifier(**params)\n",
    "model4.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = model4.predict(X_train)\n",
    "acc_train = (y_train == y_train_pred).sum() / len(y_train)\n",
    "\n",
    "y_test_pred = model4.predict(X_test)\n",
    "acc_test = (y_test == y_test_pred).sum() / len(y_test)\n",
    "\n",
    "acc_train, acc_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. corss-session validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-02-28T13:09:10.092758Z",
     "iopub.status.busy": "2024-02-28T13:09:10.092117Z",
     "iopub.status.idle": "2024-02-28T13:09:10.800084Z",
     "shell.execute_reply": "2024-02-28T13:09:10.799215Z",
     "shell.execute_reply.started": "2024-02-28T13:09:10.092730Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded nothing_orginal_v3(500).npy: Shape=(500,), Dtype=object\n",
      "Loaded left_orginal_v3(500).npy: Shape=(500,), Dtype=object\n",
      "Loaded right_orginal_v3(500).npy: Shape=(500,), Dtype=object\n",
      "Loaded leg_orginal_v3(500).npy: Shape=(500,), Dtype=object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pai/lib/python3.11/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 1024 is greater than input length  = 500, using nperseg = 500\n",
      "  warnings.warn('nperseg = {0:d} is greater than input length '\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:359: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/workspace/Baseline/lyh_data/session2/Feature_extracted\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from feature_engineering import *\n",
    "from load_lyh_data import *\n",
    "\n",
    "config_se2 = {\n",
    "    'fs_raw' : 250,  # 原始数据采样频率\n",
    "    'fs_down' : 125,  # 下采样频率 \n",
    "    't_of_trial' : 4, # 一个trial的时间, 单位：s\n",
    "    'start': 2, # trial有效点起始index\n",
    "    'end': 2+1000, # trial有效点结尾index\n",
    "    'channel_index_list' : np.arange(2, 2+14),\n",
    "    'file_label_dict' : {\n",
    "        \"nothing_orginal_v3(500).npy\": 3,\n",
    "        \"left_orginal_v3(500).npy\": 0,\n",
    "        \"right_orginal_v3(500).npy\": 1,\n",
    "        \"leg_orginal_v3(500).npy\": 2,\n",
    "        }, # 字典类型, 为data_dict的key到label的映射\n",
    "    'n_Classes' : 4, #类别数目\n",
    "    'pre_preparation_save_path': '/mnt/workspace/Baseline/lyh_data/session2/Standard_input', # 预处理结果保存路径\n",
    "    'Bandpass_filter_params' : { 'lowcut' : 0.05, 'highcut' : 40, 'fs' : 125, 'order' : 2 }, # 带通滤波器参数\n",
    "    'Preprocessed_save_path' : \"/mnt/workspace/Baseline/lyh_data/session2/Preprocessed_data\",\n",
    "    'Feature_extracted_save_path' : '/mnt/workspace/Baseline/lyh_data/session2/Feature_extracted'\n",
    "}\n",
    "\n",
    "session2_path = os.getcwd() + \"/lyh_data/session2/raw_data\"\n",
    "selected_file_list = [\"nothing_orginal_v3(500).npy\", \"left_orginal_v3(500).npy\", \\\n",
    "        \"right_orginal_v3(500).npy\", \"leg_orginal_v3(500).npy\"]\n",
    "raw_data_dict_se2 = read_folder_npy_data(session2_path, selected_file_list)\n",
    "# for key, value in raw_data_dict_se2.items():\n",
    "#     print(key, raw_data_dict_se2[key].shape)\n",
    "standard_input_data_list_se2 = pre_preparation(raw_data_dict_se2, config_se2)\n",
    "preprocessed_data_se2 = pre_processing(standard_input_data_list_se2, config_se2)\n",
    "dataset_np_se2 = np.array(preprocessed_data_se2)\n",
    "dataset_np_se2 = dataset_np_se2.reshape(-1, 14, 500)\n",
    "label_list_se2 = [0] * 500 + [1] * 500 + [2] * 500 + [3] * 500\n",
    "data_df_se2 = pd.DataFrame({'raw data': list(dataset_np_se2), 'label': label_list_se2})\n",
    "feature_extracting(data_df_se2, config_se2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T13:13:38.553077Z",
     "iopub.status.busy": "2024-02-28T13:13:38.552332Z",
     "iopub.status.idle": "2024-02-28T13:13:38.600116Z",
     "shell.execute_reply": "2024-02-28T13:13:38.599049Z",
     "shell.execute_reply.started": "2024-02-28T13:13:38.553046Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw data</th>\n",
       "      <th>label</th>\n",
       "      <th>zero_crossing_rate_c1</th>\n",
       "      <th>mean_c1</th>\n",
       "      <th>std_c1</th>\n",
       "      <th>first_order_diff_c1</th>\n",
       "      <th>second_order_diff_c1</th>\n",
       "      <th>zero_crossing_rate_c2</th>\n",
       "      <th>mean_c2</th>\n",
       "      <th>std_c2</th>\n",
       "      <th>...</th>\n",
       "      <th>delta_c13</th>\n",
       "      <th>theta_c13</th>\n",
       "      <th>alpha_c13</th>\n",
       "      <th>beta_c13</th>\n",
       "      <th>gamma_c13</th>\n",
       "      <th>delta_c14</th>\n",
       "      <th>theta_c14</th>\n",
       "      <th>alpha_c14</th>\n",
       "      <th>beta_c14</th>\n",
       "      <th>gamma_c14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-171.92995397467593, -181.74033097702963, -1...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.042</td>\n",
       "      <td>107.731969</td>\n",
       "      <td>185.775271</td>\n",
       "      <td>7.594397</td>\n",
       "      <td>7.586411</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-11.025577</td>\n",
       "      <td>91.180513</td>\n",
       "      <td>...</td>\n",
       "      <td>1639.018452</td>\n",
       "      <td>174.971362</td>\n",
       "      <td>136.149951</td>\n",
       "      <td>35.013434</td>\n",
       "      <td>14.632055</td>\n",
       "      <td>4870.985397</td>\n",
       "      <td>160.828484</td>\n",
       "      <td>103.684852</td>\n",
       "      <td>60.626268</td>\n",
       "      <td>20.026094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[-30.121299914698152, -24.106581923264933, -2...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-37.488891</td>\n",
       "      <td>135.927983</td>\n",
       "      <td>7.046242</td>\n",
       "      <td>7.229164</td>\n",
       "      <td>0.018</td>\n",
       "      <td>10.854331</td>\n",
       "      <td>61.111281</td>\n",
       "      <td>...</td>\n",
       "      <td>1665.550478</td>\n",
       "      <td>58.505241</td>\n",
       "      <td>43.228221</td>\n",
       "      <td>28.384698</td>\n",
       "      <td>12.173210</td>\n",
       "      <td>1133.777647</td>\n",
       "      <td>101.500896</td>\n",
       "      <td>49.663985</td>\n",
       "      <td>47.602825</td>\n",
       "      <td>16.372983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[-155.5794131919552, -174.5132547430685, -174...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-125.399781</td>\n",
       "      <td>444.205181</td>\n",
       "      <td>10.773724</td>\n",
       "      <td>8.910344</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-121.558822</td>\n",
       "      <td>104.092615</td>\n",
       "      <td>...</td>\n",
       "      <td>3447.098512</td>\n",
       "      <td>312.548469</td>\n",
       "      <td>231.488804</td>\n",
       "      <td>74.438022</td>\n",
       "      <td>29.395010</td>\n",
       "      <td>5150.666712</td>\n",
       "      <td>178.931276</td>\n",
       "      <td>221.866543</td>\n",
       "      <td>80.819852</td>\n",
       "      <td>34.082534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 142 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            raw data  label  \\\n",
       "0  [[-171.92995397467593, -181.74033097702963, -1...      0   \n",
       "1  [[-30.121299914698152, -24.106581923264933, -2...      0   \n",
       "2  [[-155.5794131919552, -174.5132547430685, -174...      0   \n",
       "\n",
       "   zero_crossing_rate_c1     mean_c1      std_c1  first_order_diff_c1  \\\n",
       "0                  0.042  107.731969  185.775271             7.594397   \n",
       "1                  0.006  -37.488891  135.927983             7.046242   \n",
       "2                  0.002 -125.399781  444.205181            10.773724   \n",
       "\n",
       "   second_order_diff_c1  zero_crossing_rate_c2     mean_c2      std_c2  ...  \\\n",
       "0              7.586411                  0.010  -11.025577   91.180513  ...   \n",
       "1              7.229164                  0.018   10.854331   61.111281  ...   \n",
       "2              8.910344                  0.022 -121.558822  104.092615  ...   \n",
       "\n",
       "     delta_c13   theta_c13   alpha_c13   beta_c13  gamma_c13    delta_c14  \\\n",
       "0  1639.018452  174.971362  136.149951  35.013434  14.632055  4870.985397   \n",
       "1  1665.550478   58.505241   43.228221  28.384698  12.173210  1133.777647   \n",
       "2  3447.098512  312.548469  231.488804  74.438022  29.395010  5150.666712   \n",
       "\n",
       "    theta_c14   alpha_c14   beta_c14  gamma_c14  \n",
       "0  160.828484  103.684852  60.626268  20.026094  \n",
       "1  101.500896   49.663985  47.602825  16.372983  \n",
       "2  178.931276  221.866543  80.819852  34.082534  \n",
       "\n",
       "[3 rows x 142 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df_se2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.9575)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构造数据集\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 打乱数据集\n",
    "shuffled_df_se2 = data_df_se2.sample(frac=1, random_state=42)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "train_df, test_df = train_test_split(shuffled_df_se2, test_size=0.2, random_state=42)\n",
    "y_train = train_df['label']\n",
    "X_train = train_df.drop(columns=['label', 'raw data'])\n",
    "y_test = test_df['label']\n",
    "X_test = test_df.drop(columns=['label', 'raw data'])\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "lgm = XGBClassifier()\n",
    "lgm.fit(X_train, y_train)\n",
    "lgm.save_model(\"/mnt/workspace/Baseline/classifier/model_to_load/xgb_4s_se2.json\")\n",
    "\n",
    "# 手工特征提取+xgboost\n",
    "y_train_pred = lgm.predict(X_train)\n",
    "acc_train = (y_train == y_train_pred).sum() / len(y_train)\n",
    "\n",
    "y_test_pred = lgm.predict(X_test)\n",
    "acc_test = (y_test == y_test_pred).sum() / len(y_test)\n",
    "acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross session vaildation(session1 train, session2 test):\n",
      "1.0 0.3625\n",
      "cross session vaildation(session2 train, session1 test):\n",
      "1.0 0.41333333333333333\n"
     ]
    }
   ],
   "source": [
    "# cross session\n",
    "se1_path = '/mnt/workspace/Baseline/lyh_data/session1/Feature_extracted/feature_extracted.csv'\n",
    "data_df_se1 = pd.read_csv(se1_path)\n",
    "\n",
    "shuffled_df_se1 = data_df_se1.sample(frac=1)\n",
    "shuffled_df_se2 = data_df_se2.sample(frac=1)\n",
    "\n",
    "y_se1 = shuffled_df_se1['label']\n",
    "X_se1 = shuffled_df_se1.drop(columns=['raw data', 'label'])\n",
    "y_se2 = shuffled_df_se2['label']\n",
    "X_se2 = shuffled_df_se2.drop(columns=['raw data', 'label'])\n",
    "\n",
    "lgm1 = XGBClassifier()\n",
    "lgm1.fit(X_se1, y_se1)\n",
    "lgm1.save_model(\"/mnt/workspace/Baseline/classifier/model_to_load/xgb_4s_se1.json\")\n",
    "\n",
    "# session1 train, session2 test\n",
    "y_train_pred = lgm1.predict(X_se1)\n",
    "acc_train = (y_se1 == y_train_pred).sum() / len(y_se1)\n",
    "\n",
    "y_test_pred = lgm1.predict(X_se2)\n",
    "acc_test = (y_se2 == y_test_pred).sum() / len(y_se2)\n",
    "print(\"cross session vaildation(session1 train, session2 test):\")\n",
    "print(acc_train, acc_test)\n",
    "\n",
    "lgm2 = XGBClassifier()\n",
    "lgm2.fit(X_se2, y_se2)\n",
    "lgm2.save_model(\"/mnt/workspace/Baseline/classifier/model_to_load/xgb_4s_se2.json\")\n",
    "\n",
    "# session1 train, session2 test\n",
    "y_train_pred = lgm2.predict(X_se2)\n",
    "acc_train = (y_se2 == y_train_pred).sum() / len(y_se2)\n",
    "\n",
    "y_test_pred = lgm2.predict(X_se1)\n",
    "acc_test = (y_se1 == y_test_pred).sum() / len(y_se1)\n",
    "print(\"cross session vaildation(session2 train, session1 test):\")\n",
    "print(acc_train, acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2560 entries, 870 to 1658\n",
      "Columns: 142 entries, raw data to gamma_c14\n",
      "dtypes: float64(140), int64(1), object(1)\n",
      "memory usage: 2.8+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0, 0.94375)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_se1, test_df_se1 = train_test_split(shuffled_df_se1, \\\n",
    "    test_size=0.2, random_state=42)\n",
    "train_df_se2, test_df_se2 = train_test_split(shuffled_df_se2, \\\n",
    "    test_size=0.2, random_state=42)\n",
    "train_df = pd.concat([train_df_se1, train_df_se2], axis=0)\n",
    "test_df = pd.concat([test_df_se1, test_df_se2], axis=0)\n",
    "y_train = train_df['label']\n",
    "X_train = train_df.drop(columns=['label', 'raw data'])\n",
    "y_test = test_df['label']\n",
    "X_test = test_df.drop(columns=['label', 'raw data'])\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "lgm = XGBClassifier()\n",
    "lgm.fit(X_train, y_train)\n",
    "lgm.save_model(\"/mnt/workspace/Baseline/classifier/model_to_load/xgb_4s_se1se2.json\")\n",
    "\n",
    "# mixed-session\n",
    "y_train_pred = lgm.predict(X_train)\n",
    "acc_train = (y_train == y_train_pred).sum() / len(y_train)\n",
    "\n",
    "y_test_pred = lgm.predict(X_test)\n",
    "acc_test = (y_test == y_test_pred).sum() / len(y_test)\n",
    "acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.9833333333333333)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构造数据集\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_df_se1 = data_df_se1.drop(data_df_se1[data_df_se1['label'] == 3].index)\n",
    "# 打乱数据集\n",
    "shuffled_df_se1 = data_df_se1.sample(frac=1, random_state=42)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "train_df, test_df = train_test_split(shuffled_df_se1, test_size=0.2, random_state=42)\n",
    "y_train = train_df['label']\n",
    "X_train = train_df.drop(columns=['label', 'raw data'])\n",
    "y_test = test_df['label']\n",
    "X_test = test_df.drop(columns=['label', 'raw data'])\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "lgm = XGBClassifier()\n",
    "lgm.fit(X_train, y_train)\n",
    "\n",
    "# 手工特征提取+xgboost\n",
    "y_train_pred = lgm.predict(X_train)\n",
    "acc_train = (y_train == y_train_pred).sum() / len(y_train)\n",
    "\n",
    "y_test_pred = lgm.predict(X_test)\n",
    "acc_test = (y_test == y_test_pred).sum() / len(y_test)\n",
    "acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.98)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构造数据集\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_df_se2 = data_df_se2.drop(data_df_se2[data_df_se2['label'] == 3].index)\n",
    "# 打乱数据集\n",
    "data_df_se2 = data_df_se2.sample(frac=1, random_state=42)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "train_df, test_df = train_test_split(data_df_se2, test_size=0.2, random_state=42)\n",
    "y_train = train_df['label']\n",
    "X_train = train_df.drop(columns=['label', 'raw data'])\n",
    "y_test = test_df['label']\n",
    "X_test = test_df.drop(columns=['label', 'raw data'])\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "lgm = XGBClassifier()\n",
    "lgm.fit(X_train, y_train)\n",
    "\n",
    "# 手工特征提取+xgboost\n",
    "y_train_pred = lgm.predict(X_train)\n",
    "acc_train = (y_train == y_train_pred).sum() / len(y_train)\n",
    "\n",
    "y_test_pred = lgm.predict(X_test)\n",
    "acc_test = (y_test == y_test_pred).sum() / len(y_test)\n",
    "acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross session vaildation(session1 train, session2 test):\n",
      "1.0 0.464\n",
      "cross session vaildation(session2 train, session1 test):\n",
      "1.0 0.4255555555555556\n"
     ]
    }
   ],
   "source": [
    "# cross session\n",
    "data_df_se1 = data_df_se1.drop(data_df_se1[data_df_se1['label'] == 3].index)\n",
    "data_df_se2 = data_df_se2.drop(data_df_se2[data_df_se2['label'] == 3].index)\n",
    "shuffled_df_se1 = data_df_se1.sample(frac=1, random_state=42)\n",
    "shuffled_df_se2 = data_df_se2.sample(frac=1, random_state=42)\n",
    "\n",
    "y_se1 = shuffled_df_se1['label']\n",
    "X_se1 = shuffled_df_se1.drop(columns=['raw data', 'label'])\n",
    "y_se2 = shuffled_df_se2['label']\n",
    "X_se2 = shuffled_df_se2.drop(columns=['raw data', 'label'])\n",
    "\n",
    "lgm1 = XGBClassifier()\n",
    "lgm1.fit(X_se1, y_se1)\n",
    "\n",
    "# session1 train, session2 test\n",
    "y_train_pred = lgm1.predict(X_se1)\n",
    "acc_train = (y_se1 == y_train_pred).sum() / len(y_se1)\n",
    "\n",
    "y_test_pred = lgm1.predict(X_se2)\n",
    "acc_test = (y_se2 == y_test_pred).sum() / len(y_se2)\n",
    "print(\"cross session vaildation(session1 train, session2 test):\")\n",
    "print(acc_train, acc_test)\n",
    "\n",
    "lgm2 = XGBClassifier()\n",
    "lgm2.fit(X_se2, y_se2)\n",
    "\n",
    "# session1 train, session2 test\n",
    "y_train_pred = lgm2.predict(X_se2)\n",
    "acc_train = (y_se2 == y_train_pred).sum() / len(y_se2)\n",
    "\n",
    "y_test_pred = lgm2.predict(X_se1)\n",
    "acc_test = (y_se1 == y_test_pred).sum() / len(y_se1)\n",
    "print(\"cross session vaildation(session2 train, session1 test):\")\n",
    "print(acc_train, acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.9708333333333333)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df_se1 = data_df_se1.drop(data_df_se1[data_df_se1['label'] == 3].index)\n",
    "data_df_se2 = data_df_se2.drop(data_df_se2[data_df_se2['label'] == 3].index)\n",
    "shuffled_df_se1 = data_df_se1.sample(frac=1, random_state=42)\n",
    "shuffled_df_se2 = data_df_se2.sample(frac=1, random_state=42)\n",
    "\n",
    "train_df_se1, test_df_se1 = train_test_split(shuffled_df_se1, \\\n",
    "    test_size=0.2, random_state=42)\n",
    "train_df_se2, test_df_se2 = train_test_split(shuffled_df_se2, \\\n",
    "    test_size=0.2, random_state=42)\n",
    "train_df = pd.concat([train_df_se1, train_df_se2], axis=0)\n",
    "test_df = pd.concat([test_df_se1, test_df_se2], axis=0)\n",
    "y_train = train_df['label']\n",
    "X_train = train_df.drop(columns=['label', 'raw data'])\n",
    "y_test = test_df['label']\n",
    "X_test = test_df.drop(columns=['label', 'raw data'])\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "lgm = XGBClassifier()\n",
    "lgm.fit(X_train, y_train)\n",
    "\n",
    "# mixed-session\n",
    "y_train_pred = lgm.predict(X_train)\n",
    "acc_train = (y_train == y_train_pred).sum() / len(y_train)\n",
    "\n",
    "y_test_pred = lgm.predict(X_test)\n",
    "acc_test = (y_test == y_test_pred).sum() / len(y_test)\n",
    "acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.882962962962963)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df_se1 = data_df_se1.drop(data_df_se1[data_df_se1['label'] == 3].index)\n",
    "data_df_se2 = data_df_se2.drop(data_df_se2[data_df_se2['label'] == 3].index)\n",
    "shuffled_df_se1 = data_df_se1.sample(frac=1, random_state=42)\n",
    "shuffled_df_se2 = data_df_se2.sample(frac=1, random_state=42)\n",
    "\n",
    "test_df_se2, train_df_se2 = train_test_split(shuffled_df_se2, \\\n",
    "    test_size=0.1, random_state=42)\n",
    "train_df = pd.concat([data_df_se1, train_df_se2], axis=0)\n",
    "test_df = test_df_se2\n",
    "y_train = train_df['label']\n",
    "X_train = train_df.drop(columns=['label', 'raw data'])\n",
    "y_test = test_df['label']\n",
    "X_test = test_df.drop(columns=['label', 'raw data'])\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "lgm = XGBClassifier()\n",
    "lgm.fit(X_train, y_train)\n",
    "\n",
    "# mixed-session\n",
    "y_train_pred = lgm.predict(X_train)\n",
    "acc_train = (y_train == y_train_pred).sum() / len(y_train)\n",
    "\n",
    "y_test_pred = lgm.predict(X_test)\n",
    "acc_test = (y_test == y_test_pred).sum() / len(y_test)\n",
    "acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1050,), (1350,))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. bcic数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results in s001::: acc in train: 1.0, acc in test: 0.49\n",
      "Results in s002::: acc in train: 1.0, acc in test: 0.22\n",
      "Results in s003::: acc in train: 1.0, acc in test: 0.56\n",
      "Results in s004::: acc in train: 1.0, acc in test: 0.4\n",
      "Total resuts::: acc in train: 1.0, acc in test: 0.42\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "subs1 = ['s001', 's002', 's003', 's004']\n",
    "subs2 = ['e001', 'e002', 'e003', 'e004']\n",
    "\n",
    "csv_file_floder = '/mnt/workspace/Baseline/bci42a_data/Feature_extracted'\n",
    "acc_t, acc_e = [], []\n",
    "\n",
    "for i in range(len(subs1)):\n",
    "    train_set = pd.read_csv(csv_file_floder + '/' + subs1[i] + '.csv')\n",
    "    test_set = pd.read_csv(csv_file_floder + '/' + subs2[i] + '.csv')\n",
    "    train_set = train_set.sample(frac=1, random_state=42)\n",
    "    test_set = test_set.sample(frac=1, random_state=42)\n",
    "\n",
    "    y_se1 = train_set['label']\n",
    "    X_se1 = train_set.drop(columns=['raw data', 'label'])\n",
    "    y_se2 = test_set['label']\n",
    "    X_se2 = test_set.drop(columns=['raw data', 'label'])\n",
    "\n",
    "    xgb = XGBClassifier()\n",
    "    xgb.fit(X_se1, y_se1)\n",
    "\n",
    "    # session1 train, session2 test\n",
    "    y_train_pred = xgb.predict(X_se1)\n",
    "    acc_train = (y_se1 == y_train_pred).sum() / len(y_se1)\n",
    "\n",
    "    y_test_pred = xgb.predict(X_se2)\n",
    "    acc_test = (y_se2 == y_test_pred).sum() / len(y_se2)\n",
    "\n",
    "    acc_t.append(round(acc_train, 2))\n",
    "    acc_e.append(round(acc_test, 2))\n",
    "\n",
    "    print(f\"Results in {subs1[i]}::: acc in train: {round(acc_train, 2)}, acc in test: {round(acc_test, 2)}\")\n",
    "\n",
    "print(f\"Total resuts::: acc in train: {round(sum(acc_t) / len(acc_t), 2)}, acc in test: {round(sum(acc_e) / len(acc_e), 2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results in s001::: acc in train: 1.0, acc in test: 0.5\n",
      "Results in s002::: acc in train: 1.0, acc in test: 0.2\n",
      "Results in s003::: acc in train: 1.0, acc in test: 0.54\n",
      "Results in s004::: acc in train: 1.0, acc in test: 0.4\n",
      "Total resuts::: acc in train: 1.0, acc in test: 0.41\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "subs1 = ['s001', 's002', 's003', 's004']\n",
    "subs2 = ['e001', 'e002', 'e003', 'e004']\n",
    "\n",
    "csv_file_floder = '/mnt/workspace/Baseline/bci42a_data/Feature_extracted'\n",
    "acc_t, acc_e = [], []\n",
    "\n",
    "## mix-session\n",
    "for i in range(len(subs1)):\n",
    "    train_set = pd.read_csv(csv_file_floder + '/' + subs1[i] + '.csv')\n",
    "    test_set = pd.read_csv(csv_file_floder + '/' + subs2[i] + '.csv')\n",
    "    train_set = train_set.sample(frac=1, random_state=42)\n",
    "    test_set = test_set.sample(frac=1, random_state=42)\n",
    "\n",
    "    test_df_se2, train_df_se2 = train_test_split(test_set, \\\n",
    "    test_size=0.1, random_state=42)\n",
    "    train_set = pd.concat([train_set, train_df_se2], axis=0)\n",
    "    test_set = test_df_se2\n",
    "\n",
    "    y_se1 = train_set['label']\n",
    "    X_se1 = train_set.drop(columns=['raw data', 'label'])\n",
    "    y_se2 = test_set['label']\n",
    "    X_se2 = test_set.drop(columns=['raw data', 'label'])\n",
    "\n",
    "    xgb = XGBClassifier()\n",
    "    xgb.fit(X_se1, y_se1)\n",
    "\n",
    "    # session1 train, session2 test\n",
    "    y_train_pred = xgb.predict(X_se1)\n",
    "    acc_train = (y_se1 == y_train_pred).sum() / len(y_se1)\n",
    "\n",
    "    y_test_pred = xgb.predict(X_se2)\n",
    "    acc_test = (y_se2 == y_test_pred).sum() / len(y_se2)\n",
    "\n",
    "    acc_t.append(round(acc_train, 2))\n",
    "    acc_e.append(round(acc_test, 2))\n",
    "\n",
    "    print(f\"Results in {subs1[i]}::: acc in train: {round(acc_train, 2)}, acc in test: {round(acc_test, 2)}\")\n",
    "\n",
    "print(f\"Total resuts::: acc in train: {round(sum(acc_t) / len(acc_t), 2)}, acc in test: {round(sum(acc_e) / len(acc_e), 2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results in s001::: acc in train: 1.0, acc in test: 0.57\n",
      "Results in s002::: acc in train: 1.0, acc in test: 0.28\n",
      "Results in s003::: acc in train: 1.0, acc in test: 0.41\n",
      "Results in s004::: acc in train: 1.0, acc in test: 0.44\n",
      "Total resuts::: acc in train: 1.0, acc in test: 0.42\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "subs1 = ['s001', 's002', 's003', 's004']\n",
    "subs2 = ['e001', 'e002', 'e003', 'e004']\n",
    "\n",
    "csv_file_floder = '/mnt/workspace/Baseline/bci42a_data/Feature_extracted'\n",
    "acc_t, acc_e = [], []\n",
    "\n",
    "## mix-session\n",
    "for i in range(len(subs1)):\n",
    "    train_set = pd.read_csv(csv_file_floder + '/' + subs1[i] + '.csv')\n",
    "    test_set = pd.read_csv(csv_file_floder + '/' + subs2[i] + '.csv')\n",
    "    train_set = train_set.sample(frac=1, random_state=42)\n",
    "    test_set = test_set.sample(frac=1, random_state=42)\n",
    "\n",
    "    train_df_se2, test_df_se2 = train_test_split(test_set, \\\n",
    "    test_size=0.2, random_state=42)\n",
    "    train_df_se1, test_df_se1 = train_test_split(train_set, \\\n",
    "    test_size=0.2, random_state=42)\n",
    "    train_set = pd.concat([train_df_se1, train_df_se2], axis=0)\n",
    "    test_set = pd.concat([test_df_se1, test_df_se2], axis=0)\n",
    "\n",
    "    y_se1 = train_set['label']\n",
    "    X_se1 = train_set.drop(columns=['raw data', 'label'])\n",
    "    y_se2 = test_set['label']\n",
    "    X_se2 = test_set.drop(columns=['raw data', 'label'])\n",
    "\n",
    "    xgb = XGBClassifier()\n",
    "    xgb.fit(X_se1, y_se1)\n",
    "\n",
    "    # session1 train, session2 test\n",
    "    y_train_pred = xgb.predict(X_se1)\n",
    "    acc_train = (y_se1 == y_train_pred).sum() / len(y_se1)\n",
    "\n",
    "    y_test_pred = xgb.predict(X_se2)\n",
    "    acc_test = (y_se2 == y_test_pred).sum() / len(y_se2)\n",
    "\n",
    "    acc_t.append(round(acc_train, 2))\n",
    "    acc_e.append(round(acc_test, 2))\n",
    "\n",
    "    print(f\"Results in {subs1[i]}::: acc in train: {round(acc_train, 2)}, acc in test: {round(acc_test, 2)}\")\n",
    "\n",
    "print(f\"Total resuts::: acc in train: {round(sum(acc_t) / len(acc_t), 2)}, acc in test: {round(sum(acc_e) / len(acc_e), 2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******In-session Validation (Session 1)******\n",
      "Results in s001::: acc in train: 1.0, acc in test: 0.43\n",
      "Results in s002::: acc in train: 1.0, acc in test: 0.36\n",
      "Results in s003::: acc in train: 1.0, acc in test: 0.5\n",
      "Results in s004::: acc in train: 1.0, acc in test: 0.26\n",
      "Total resuts::: acc in train: 1.0, acc in test: 0.39\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "subs1 = ['s001', 's002', 's003', 's004']\n",
    "subs2 = ['e001', 'e002', 'e003', 'e004']\n",
    "\n",
    "csv_file_floder = '/mnt/workspace/Baseline/bci42a_data/Feature_extracted'\n",
    "acc_t, acc_e = [], []\n",
    "\n",
    "print(\"******In-session Validation (Session 1)******\")\n",
    "\n",
    "## in-session\n",
    "for i in range(len(subs1)):\n",
    "    train_set = pd.read_csv(csv_file_floder + '/' + subs1[i] + '.csv')\n",
    "    # test_set = pd.read_csv(csv_file_floder + '/' + subs2[i] + '.csv')\n",
    "    train_set = train_set.sample(frac=1, random_state=42)\n",
    "\n",
    "    train_set, test_set = train_test_split(train_set, \\\n",
    "    test_size=0.2, random_state=42)\n",
    "\n",
    "    y_se1 = train_set['label']\n",
    "    X_se1 = train_set.drop(columns=['raw data', 'label'])\n",
    "    y_se2 = test_set['label']\n",
    "    X_se2 = test_set.drop(columns=['raw data', 'label'])\n",
    "\n",
    "    xgb = XGBClassifier()\n",
    "    xgb.fit(X_se1, y_se1)\n",
    "\n",
    "    # session1 train, session2 test\n",
    "    y_train_pred = xgb.predict(X_se1)\n",
    "    acc_train = (y_se1 == y_train_pred).sum() / len(y_se1)\n",
    "\n",
    "    y_test_pred = xgb.predict(X_se2)\n",
    "    acc_test = (y_se2 == y_test_pred).sum() / len(y_se2)\n",
    "\n",
    "    acc_t.append(round(acc_train, 2))\n",
    "    acc_e.append(round(acc_test, 2))\n",
    "\n",
    "    print(f\"Results in {subs1[i]}::: acc in train: {round(acc_train, 2)}, acc in test: {round(acc_test, 2)}\")\n",
    "\n",
    "print(f\"Total resuts::: acc in train: {round(sum(acc_t) / len(acc_t), 2)}, acc in test: {round(sum(acc_e) / len(acc_e), 2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******In-session Validation (Session 2)******\n",
      "Results in s001::: acc in train: 1.0, acc in test: 0.5\n",
      "Results in s002::: acc in train: 1.0, acc in test: 0.26\n",
      "Results in s003::: acc in train: 1.0, acc in test: 0.48\n",
      "Results in s004::: acc in train: 1.0, acc in test: 0.47\n",
      "Total resuts::: acc in train: 1.0, acc in test: 0.43\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "subs1 = ['s001', 's002', 's003', 's004']\n",
    "subs2 = ['e001', 'e002', 'e003', 'e004']\n",
    "\n",
    "csv_file_floder = '/mnt/workspace/Baseline/bci42a_data/Feature_extracted'\n",
    "acc_t, acc_e = [], []\n",
    "\n",
    "print(\"******In-session Validation (Session 2)******\")\n",
    "\n",
    "## in-session\n",
    "for i in range(len(subs1)):\n",
    "    # train_set = pd.read_csv(csv_file_floder + '/' + subs1[i] + '.csv')\n",
    "    # test_set = pd.read_csv(csv_file_floder + '/' + subs2[i] + '.csv')\n",
    "    train_set = pd.read_csv(csv_file_floder + '/' + subs2[i] + '.csv')\n",
    "    train_set = train_set.sample(frac=1, random_state=42)\n",
    "\n",
    "    train_set, test_set = train_test_split(train_set, \\\n",
    "    test_size=0.2, random_state=42)\n",
    "\n",
    "    y_se1 = train_set['label']\n",
    "    X_se1 = train_set.drop(columns=['raw data', 'label'])\n",
    "    y_se2 = test_set['label']\n",
    "    X_se2 = test_set.drop(columns=['raw data', 'label'])\n",
    "\n",
    "    xgb = XGBClassifier()\n",
    "    xgb.fit(X_se1, y_se1)\n",
    "\n",
    "    # session1 train, session2 test\n",
    "    y_train_pred = xgb.predict(X_se1)\n",
    "    acc_train = (y_se1 == y_train_pred).sum() / len(y_se1)\n",
    "\n",
    "    y_test_pred = xgb.predict(X_se2)\n",
    "    acc_test = (y_se2 == y_test_pred).sum() / len(y_se2)\n",
    "\n",
    "    acc_t.append(round(acc_train, 2))\n",
    "    acc_e.append(round(acc_test, 2))\n",
    "\n",
    "    print(f\"Results in {subs1[i]}::: acc in train: {round(acc_train, 2)}, acc in test: {round(acc_test, 2)}\")\n",
    "\n",
    "print(f\"Total resuts::: acc in train: {round(sum(acc_t) / len(acc_t), 2)}, acc in test: {round(sum(acc_e) / len(acc_e), 2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-s数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import butter, filtfilt, iirnotch, lfilter\n",
    "from copy import deepcopy\n",
    "from entropy import *\n",
    "import entropy\n",
    "from scipy.signal import welch\n",
    "from scipy.io import loadmat, savemat\n",
    "\n",
    "\n",
    "class EEG_DataLoader:\n",
    "    '''对raw data进行数据预处理, 包括下采样、选择通道、滤波、时域特征提取(过零率、均值、标准差、一阶\\二阶差分)、\n",
    "    频域特征提取(传统五频道法)\n",
    "    X_raw: raw data, 类型: List[二维numpy arrray, shape=(channels, time_points)]\n",
    "    y : X_raw 对应的标签\n",
    "    config: 配置信息\n",
    "    data_df: 预处理之后的数据, 其中两个colums=['raw data', 'label']不用来训练, 其他的colums都是提取出来的特征,\n",
    "    用来训练.\n",
    "    API: self.feature_engineering(): 返回一个datafrmae类型的数据, 可用来训练。\n",
    "    '''\n",
    "    def __init__(self, X_raw, y, config):\n",
    "        self.X_raw = X_raw\n",
    "        self.y = y\n",
    "        self.config = config\n",
    "        self.standard_input_data = []\n",
    "        self.pre_processed_data = []\n",
    "        self.data_df = None\n",
    "\n",
    "    def pre_preparation(self):\n",
    "        '''\n",
    "        预处理: 下采样+选择通道\n",
    "        '''\n",
    "\n",
    "        def down_sample(data, start, end, fs_raw, fs_down):\n",
    "            '''\n",
    "            下采样\n",
    "            \n",
    "            param---\n",
    "            data: numpy数组, 一个trial, shape = (n_channels, n_times)\n",
    "            start: trial有效点起始index\n",
    "            end: trial有效点结尾index\n",
    "            \n",
    "            return---\n",
    "            data: 下采样之后的数组\n",
    "            '''\n",
    "            \n",
    "            step = fs_raw // fs_down\n",
    "            data = data[:, start: end: step]\n",
    "            \n",
    "            return data\n",
    "\n",
    "        def selected_channel(data, channel_index_list):\n",
    "            '''\n",
    "            选择有效的通道\n",
    "            \n",
    "            param---\n",
    "            data: numpy数组, 一个trial, shape = (n_channels, n_times)\n",
    "            channel_index_list: list类型, 有效通道的index\n",
    "            '''\n",
    "            return data[channel_index_list, :]\n",
    "\n",
    "        for trial in self.X_raw:\n",
    "            trial = down_sample(np.array(trial), self.config['start'], self.config['end'], self.config['fs_raw'], config['fs_down'])\n",
    "            trial = selected_channel(trial, self.config['channel_index_list'])\n",
    "            self.standard_input_data.append(trial)\n",
    "\n",
    "        if self.config['pre_preparation_save_path'] != '':\n",
    "            if not os.path.exists(self.config['pre_preparation_save_path']):\n",
    "                os.makedirs(self.config['pre_preparation_save_path'])\n",
    "            np.save(self.config['pre_preparation_save_path'] + \"/standard_input_data\" + \".npy\", self.standard_input_data)\n",
    "\n",
    "    def pre_processing(self, butter_order = 2):\n",
    "        \n",
    "        def butter_bandpass_filter(data, config):\n",
    "            fa = 0.5 * config['fs']\n",
    "            low = config['lowcut'] / fa\n",
    "            high = config['highcut'] / fa\n",
    "            b, a = butter(config['order'], [low, high], btype='band')\n",
    "            ret = []\n",
    "            for line in data:\n",
    "                ret.append(filtfilt(b, a, line))\n",
    "            return np.array(ret)\n",
    "\n",
    "        def iirnotch_filter(data, fs = 125, Q = 30, f_cut = 50.0):\n",
    "            ret = []\n",
    "            b, a = iirnotch(f_cut, Q, fs)\n",
    "            for line in data:\n",
    "                ret.append(lfilter(b,a, line))\n",
    "            return np.array(ret)\n",
    "\n",
    "        \n",
    "        for i in range(len(self.standard_input_data)):\n",
    "            assert self.standard_input_data[i].shape == (14, 125)\n",
    "            x = butter_bandpass_filter(self.standard_input_data[i], self.config['Bandpass_filter_params'])\n",
    "            x = iirnotch_filter(x)\n",
    "            self.pre_processed_data.append(x)\n",
    "\n",
    "        if self.config['Preprocessed_save_path'] != '':\n",
    "            if not os.path.exists(self.config['Preprocessed_save_path']):\n",
    "                os.makedirs(self.config['Preprocessed_save_path'])\n",
    "            np.save(self.config['Preprocessed_save_path'] + \"/pre_processed_data\" + \".npy\", self.pre_processed_data)\n",
    "\n",
    "    def time_domain_feature(self):\n",
    "        '''\n",
    "        提取时域特征\n",
    "        '''\n",
    "        # 计算过零率\n",
    "        def zero_crossing_rate(trials):\n",
    "            '''\n",
    "            计算一个trials(二维列表)各个通道的过零率, 返回一个list\n",
    "            '''\n",
    "\n",
    "            def compute(signal):\n",
    "                '''\n",
    "                计算一维信号的过零率\n",
    "                '''\n",
    "                crossings = np.where(np.diff(np.sign(signal)))[0]\n",
    "                zero_crossing_rate = len(crossings) / len(signal)\n",
    "                return zero_crossing_rate\n",
    "\n",
    "            ret = [compute(trials[i]) for i in range(len(trials))]\n",
    "            return np.array(ret)\n",
    "        \n",
    "        # 计算均值\n",
    "\n",
    "        def calculate_channel_means(eeg_data):\n",
    "            \"\"\"\n",
    "            计算每个通道的均值。\n",
    "\n",
    "            参数：\n",
    "            - eeg_data: 二维 NumPy 数组，表示 EEG 数据，形状为 (n_channels, n_points)。\n",
    "\n",
    "            返回：\n",
    "            - channel_means: 一维 NumPy 数组，包含每个通道的均值。\n",
    "            \"\"\"\n",
    "            # 计算每个通道的均值，axis=2 表示在样本点上求均值\n",
    "            channel_means = np.mean(eeg_data, axis=1)\n",
    "\n",
    "            return channel_means\n",
    "\n",
    "        # 计算标准差\n",
    "        def calculate_channel_stds(eeg_data):\n",
    "            \"\"\"\n",
    "            计算每个通道的均值。\n",
    "\n",
    "            参数：\n",
    "            - eeg_data: 二维 NumPy 数组，表示 EEG 数据，形状为 (n_channels, n_points)。\n",
    "\n",
    "            返回：\n",
    "            - channel_means: 一维 NumPy 数组，包含每个通道的均值。\n",
    "            \"\"\"\n",
    "            # 计算每个通道的均值，axis=2 表示在样本点上求均值\n",
    "            channel_stds = np.std(eeg_data, axis=1)\n",
    "\n",
    "            return channel_stds\n",
    "        \n",
    "        def calculate_first_order_diff(eeg_data):\n",
    "\n",
    "            first_order_diff = np.sum(np.abs(np.diff(eeg_data, axis=1)), axis=1) / (eeg_data.shape[1] - 1)\n",
    "\n",
    "            return first_order_diff\n",
    "\n",
    "        def calculate_second_order_diff(eeg_data):\n",
    "            \n",
    "            second_order_diff = np.sum(np.abs(np.diff(eeg_data, n=2, axis=1)), axis=1) / (eeg_data.shape[1] - 2)\n",
    "\n",
    "            return second_order_diff\n",
    "        \n",
    "        zero_crossing_list = []\n",
    "        means_list = []\n",
    "        stds_list = []\n",
    "        first_order_diff_list = []\n",
    "        second_order_diff_list = []\n",
    "\n",
    "        for _, row in self.data_df.iterrows():\n",
    "            zero_crossing = zero_crossing_rate(row['raw data'])\n",
    "            zero_crossing_list.append(zero_crossing)\n",
    "            means = calculate_channel_means(row['raw data'])\n",
    "            means_list.append(means)\n",
    "            stds = calculate_channel_stds(row['raw data'])\n",
    "            stds_list.append(stds)\n",
    "            first_diff = calculate_first_order_diff(row['raw data'])\n",
    "            first_order_diff_list.append(first_diff)\n",
    "            second_diff = calculate_second_order_diff(row['raw data'])\n",
    "            second_order_diff_list.append(second_diff)\n",
    "        \n",
    "        nChannels = zero_crossing_list[0].shape[0]\n",
    "        for i in range(nChannels):\n",
    "            self.data_df[f'zero_crossing_rate_c{i+1}'] = np.array(zero_crossing_list)[:,i]\n",
    "            self.data_df[f'mean_c{i+1}'] = np.array(means_list)[:,i]\n",
    "            self.data_df[f'std_c{i+1}'] = np.array(stds_list)[:,i]\n",
    "            self.data_df[f'first_order_diff_c{i+1}'] = np.array(first_order_diff_list)[:,i]\n",
    "            self.data_df[f'second_order_diff_c{i+1}'] = np.array(second_order_diff_list)[:,i]\n",
    "\n",
    "    def freq_domain_feature(self):\n",
    "        '''\n",
    "        提取时域特征\n",
    "\n",
    "        参数:\n",
    "        - eeg_dataset: Dataframe类型\n",
    "        '''\n",
    "\n",
    "        def five_band_energy(eeg_data, fs=125):\n",
    "    \n",
    "            energy = []\n",
    "            \n",
    "            for eeg_signal in eeg_data:\n",
    "                # 计算功率谱密度（PSD）\n",
    "                frequencies, psd = welch(eeg_signal, fs, nperseg=1024)\n",
    "\n",
    "                # 定义频带边界\n",
    "                delta_band = (0.5, 4)\n",
    "                theta_band = (4, 8)\n",
    "                alpha_band = (8, 14)\n",
    "                beta_band = (14, 30)\n",
    "                gamma_band = (30, 60)\n",
    "\n",
    "                # 计算每个频带内的能量\n",
    "                energy.append(np.trapz(psd[(frequencies >= delta_band[0]) & (frequencies <= delta_band[1])], \\\n",
    "                    frequencies[(frequencies >= delta_band[0]) & (frequencies <= delta_band[1])]))\n",
    "                energy.append(np.trapz(psd[(frequencies >= theta_band[0]) & (frequencies <= theta_band[1])], \\\n",
    "                    frequencies[(frequencies >= theta_band[0]) & (frequencies <= theta_band[1])]))\n",
    "                energy.append(np.trapz(psd[(frequencies >= alpha_band[0]) & (frequencies <= alpha_band[1])], \\\n",
    "                    frequencies[(frequencies >= alpha_band[0]) & (frequencies <= alpha_band[1])]))\n",
    "                energy.append(np.trapz(psd[(frequencies >= beta_band[0]) & (frequencies <= beta_band[1])], \\\n",
    "                    frequencies[(frequencies >= beta_band[0]) & (frequencies <= beta_band[1])]))\n",
    "                energy.append(np.trapz(psd[(frequencies >= gamma_band[0]) & (frequencies <= gamma_band[1])], \\\n",
    "                    frequencies[(frequencies >= gamma_band[0]) & (frequencies <= gamma_band[1])]))\n",
    "\n",
    "            return np.array(energy)\n",
    "        \n",
    "        energy = []\n",
    "\n",
    "        for _, row in self.data_df.iterrows():\n",
    "            \n",
    "            energy.append(five_band_energy(row['raw data']))\n",
    "\n",
    "        freq_names = ['delta', 'theta', 'alpha', 'beta', 'gamma']\n",
    "        nFreqs = len(freq_names)\n",
    "        nChannels = energy[0].shape[0] // nFreqs\n",
    "        \n",
    "        for i in range(nChannels):\n",
    "            for j in range(nFreqs):\n",
    "                self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
    "\n",
    "    def feature_extracting(self):\n",
    "        self.data_df = pd.DataFrame({'raw data': self.pre_processed_data, 'label': self.y})\n",
    "        self.time_domain_feature()\n",
    "        self.freq_domain_feature()\n",
    "        if self.config['Feature_extracted_save_path'] != '':\n",
    "            if not os.path.exists(self.config['Feature_extracted_save_path']):\n",
    "                os.makedirs(self.config['Feature_extracted_save_path'])\n",
    "            self.data_df.to_csv(self.config['Feature_extracted_save_path'] + \"/feature_extracted_data.csv\")\n",
    "\n",
    "    def feature_engineering(self):\n",
    "        self.pre_preparation()\n",
    "        self.pre_processing()\n",
    "        self.feature_extracting()\n",
    "        return self.data_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pai/lib/python3.11/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 1024 is greater than input length  = 125, using nperseg = 125\n",
      "  warnings.warn('nperseg = {0:d} is greater than input length '\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/tmp/ipykernel_408/1093053317.py:245: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data_df[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "config = {\n",
    "        'fs_raw' : 250,  # 原始数据采样频率\n",
    "        'fs_down' : 125,  # 下采样频率 \n",
    "        't_of_trial' : 1, # 一个trial的时间, 单位：s\n",
    "        'start': 2, # trial有效点起始index\n",
    "        'end': 2+250, # trial有效点结尾index\n",
    "        'channel_index_list' : np.arange(2, 2+14),\n",
    "        'pre_preparation_save_path': '/mnt/workspace/Standard_input', # 预处理结果保存路径\n",
    "        'Bandpass_filter_params' : { 'lowcut' : 0.05, 'highcut' : 40, 'fs' : 125, 'order' : 2 }, # 带通滤波器参数\n",
    "        'Preprocessed_save_path' : \"/mnt/workspace/Preprocessed_data\",\n",
    "        'Feature_extracted_save_path' : '/mnt/workspace/Feature_extracted'\n",
    "    }\n",
    "data_path_list = [\n",
    "                   \"/mnt/workspace/Baseline/lyh_data/session2/raw_data/left_orginal_v3(500).npy\",\\\n",
    "                    \"/mnt/workspace/Baseline/lyh_data/session2/raw_data/right_orginal_v3(500).npy\",\\\n",
    "                    \"/mnt/workspace/Baseline/lyh_data/session2/raw_data/leg_orginal_v3(500).npy\" ]\n",
    "X = []\n",
    "for path in data_path_list:\n",
    "    d = np.load(path, allow_pickle=True)\n",
    "    for trial in d:\n",
    "        X.append(np.array(trial).T)\n",
    "y = [0] * 500 + [1] * 500 + [2] * 500\n",
    "data_loader = EEG_DataLoader(X, y, config)\n",
    "data_df = data_loader.feature_engineering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.9166666666666666)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构造数据集\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 打乱数据集\n",
    "shuffled_df = data_df.sample(frac=1, random_state=42)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "train_df, test_df = train_test_split(shuffled_df, test_size=0.2, random_state=42)\n",
    "y_train = train_df['label']\n",
    "X_train = train_df.drop(columns=['label', 'raw data'])\n",
    "y_test = test_df['label']\n",
    "X_test = test_df.drop(columns=['label', 'raw data'])\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "lgm = XGBClassifier()\n",
    "lgm.fit(X_train, y_train)\n",
    "lgm.save_model(\"/mnt/workspace/Baseline/classifier/model_to_load/xgb_1s_se2.json\")\n",
    "\n",
    "# 手工特征提取+xgboost\n",
    "y_train_pred = lgm.predict(X_train)\n",
    "acc_train = (y_train == y_train_pred).sum() / len(y_train)\n",
    "\n",
    "y_test_pred = lgm.predict(X_test)\n",
    "acc_test = (y_test == y_test_pred).sum() / len(y_test)\n",
    "acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T01:51:25.452557Z",
     "iopub.status.busy": "2024-03-12T01:51:25.452148Z",
     "iopub.status.idle": "2024-03-12T01:51:25.972397Z",
     "shell.execute_reply": "2024-03-12T01:51:25.971743Z",
     "shell.execute_reply.started": "2024-03-12T01:51:25.452528Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-03-12T01:56:11.254055Z",
     "iopub.status.busy": "2024-03-12T01:56:11.253641Z",
     "iopub.status.idle": "2024-03-12T01:56:11.321357Z",
     "shell.execute_reply": "2024-03-12T01:56:11.320748Z",
     "shell.execute_reply.started": "2024-03-12T01:56:11.254027Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_set_path = '/mnt/workspace/Baseline/classifier/data_set_to_load/feature_extracted_se2_1s.csv'\n",
    "data_df = pd.read_csv(data_set_path)\n",
    "\n",
    "data_df = data_df.sample(frac=1)\n",
    "train_df, test_df = train_test_split(data_df, test_size=0.85)\n",
    "y_train = train_df['label']\n",
    "X_train = train_df.drop(columns=['Unnamed: 0', 'raw data', 'label'])\n",
    "y_test = test_df['label']\n",
    "X_test = test_df.drop(columns=['Unnamed: 0', 'raw data', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******RESULT******\n",
      "Model: SVM\n",
      "Acc: train->0.3408333333333333 test->0.33666666666666667\n",
      "Confusion Matrix: [[  3   0  95]\n",
      " [  0   0 104]\n",
      " [  0   0  98]]\n"
     ]
    }
   ],
   "source": [
    "clf = SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "acc_train = accuracy_score(y_train, clf.predict(X_train))\n",
    "acc_test = accuracy_score(y_test, clf.predict(X_test))\n",
    "cm = confusion_matrix(y_pred=clf.predict(X_test), y_true=y_test)\n",
    "\n",
    "print(\"******RESULT******\")\n",
    "print(\"Model: SVM\")\n",
    "print(f\"Acc: train->{acc_train} test->{acc_test}\")\n",
    "print(f\"Confusion Matrix: {cm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-03-12T01:56:15.233227Z",
     "iopub.status.busy": "2024-03-12T01:56:15.232837Z",
     "iopub.status.idle": "2024-03-12T01:56:24.994621Z",
     "shell.execute_reply": "2024-03-12T01:56:24.994008Z",
     "shell.execute_reply.started": "2024-03-12T01:56:15.233203Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******RESULT******\n",
      "Model: XGBClassifier, nTrials for training: 225, nTrials for testing: 1275\n",
      "Acc: train->1.0 test->0.8650980392156863\n",
      "Confusion Matrix: [[368  37  30]\n",
      " [ 42 350  36]\n",
      " [  7  20 385]]\n"
     ]
    }
   ],
   "source": [
    "clf = XGBClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "acc_train = accuracy_score(y_train, clf.predict(X_train))\n",
    "acc_test = accuracy_score(y_test, clf.predict(X_test))\n",
    "cm = confusion_matrix(y_pred=clf.predict(X_test), y_true=y_test)\n",
    "\n",
    "print(\"******RESULT******\")\n",
    "print(f\"Model: XGBClassifier, nTrials for training: {y_train.shape[0]}, nTrials for testing: {y_test.shape[0]}\")\n",
    "print(f\"Acc: train->{acc_train} test->{acc_test}\")\n",
    "print(f\"Confusion Matrix: {cm}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
