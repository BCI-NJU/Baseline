{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 传统机器学习方法\n",
    "路线：前置准备 预处理 可视化 特征提取 特征选择 分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import butter, filtfilt, iirnotch, lfilter\n",
    "from copy import deepcopy\n",
    "from entropy import *\n",
    "import entropy\n",
    "from load_lyh_data import  *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 前置准备\n",
    "加载lyh的数据，这里仅仅是加载数据对数据进行分割，并未对数据进行任何处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded nothing1_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded nothing2_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded nothing3_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded left1_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded left2_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded left3_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded right1_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded right2_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded right3_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded leg1_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded leg2_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded leg3_orginal.npy: Shape=(100,), Dtype=object\n"
     ]
    }
   ],
   "source": [
    "raw_data_dict = read_folder_npy_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1027, 19)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(raw_data_dict[\"leg3_orginal.npy\"][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数设计\n",
    "\n",
    "# 下采样设置：ERP脑电有效频率<30Hz，根据脑奎斯特定理，\n",
    "# 采样率到60Hz以上即可。但为了多一些保存细节，这里降采样设置为125Hz\n",
    "fs_raw = 250\n",
    "fs_down = 125\n",
    "# 一个trial的时间\n",
    "t_of_trial = 4\n",
    "# 采样点个数\n",
    "nTime = fs_down * t_of_trial\n",
    "\n",
    "start_point_index = 2\n",
    "end_point_index = start_point_index + fs_raw * t_of_trial\n",
    "\n",
    "\n",
    "def down_sample(data, start, end, fs_raw, fs_down):\n",
    "    '''\n",
    "    下采样\n",
    "    param---\n",
    "    data: numpy数组\n",
    "    return---\n",
    "    data: 下采样之后的数组\n",
    "    '''\n",
    "    \n",
    "    step = fs_raw // fs_down\n",
    "    data = data[start: end: step]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 19)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = down_sample(np.array(raw_data_dict[\"nothing1_orginal.npy\"][0]), start_point_index, end_point_index, fs_raw, fs_down)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择channel\n",
    "channel_index_list = np.arange(2, 2+14)\n",
    "\n",
    "def selected_channel(data, channel_index_list):\n",
    "\n",
    "    return data[:, channel_index_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 14)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = selected_channel(test, channel_index_list)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_label_dict = {\n",
    "    \"nothing1_orginal.npy\": 0, \"nothing2_orginal.npy\": 0, \"nothing3_orginal.npy\": 0,\n",
    "    \"left1_orginal.npy\": 1, \"left2_orginal.npy\": 1, \"left3_orginal.npy\": 1,\n",
    "    \"right1_orginal.npy\": 2, \"right2_orginal.npy\": 2, \"right3_orginal.npy\": 2,\n",
    "    \"leg1_orginal.npy\": 3, \"leg2_orginal.npy\": 3, \"leg3_orginal.npy\": 3\n",
    "}\n",
    "save_path = os.getcwd() + \"/lyh_data/Standard_input\"\n",
    "\n",
    "def pre_preparation(data_dict, file_label_dict, save_path, nClasses=4):\n",
    "    data = [[] for _ in range(nClasses)]\n",
    "\n",
    "    for key, value in data_dict.items():\n",
    "        d = data[file_label_dict[key]]\n",
    "        for trial in value:\n",
    "            trial = down_sample(np.array(trial), start_point_index, end_point_index, fs_raw, fs_down)\n",
    "            trail = selected_channel(trial, channel_index_list)\n",
    "            d.append(trail.T)\n",
    "    \n",
    "    # save\n",
    "    for i, d in enumerate(data):\n",
    "        data[i] = np.array(d)\n",
    "        path = save_path + \"/\" + str(i) + \".npy\"\n",
    "        np.save(path, data[i])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_input_data_list = pre_preparation(raw_data_dict, file_label_dict, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 14, 500)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_input_data_list[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基线校正\n",
    "def baseline_correction(data, baseline_start, baseline_end):\n",
    "    \"\"\"\n",
    "    进行基线校正\n",
    "\n",
    "    Parameters:\n",
    "    - data: EEG 数据的二维数组，shape=(通道数, 采样点数)\n",
    "    - baseline_start: 基线起始点的索引\n",
    "    - baseline_end: 基线结束点的索引\n",
    "\n",
    "    Returns:\n",
    "    - baseline_corrected_data: 基线校正后的 EEG 数据\n",
    "    \"\"\"\n",
    "    # 计算每个通道上基线期的平均值\n",
    "    baseline_values = np.mean(data[:, baseline_start:baseline_end], axis=1, keepdims=True)\n",
    "\n",
    "    # 进行基线校正\n",
    "    baseline_corrected_data = data - baseline_values\n",
    "\n",
    "    return baseline_corrected_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基线校正前 :  [[4298.333 4301.538 4307.564 ... 4320.    4332.436 4314.872]\n",
      " [4268.462 4269.103 4268.462 ... 4282.821 4301.41  4287.051]\n",
      " [4328.462 4324.103 4328.333 ... 4351.795 4355.897 4346.667]\n",
      " ...\n",
      " [4463.077 4457.821 4469.615 ... 4502.308 4510.128 4505.897]\n",
      " [4335.641 4342.179 4357.308 ... 4426.795 4437.692 4419.231]\n",
      " [4418.846 4415.769 4421.154 ... 4469.615 4474.231 4469.744]]\n",
      "基线校正后 :  [[ 11.42274  14.62774  20.65374 ...  33.08974  45.52574  27.96174]\n",
      " [ 23.91832  24.55932  23.91832 ...  38.27732  56.86632  42.50732]\n",
      " [ 14.37734  10.01834  14.24834 ...  37.71034  41.81234  32.58234]\n",
      " ...\n",
      " [  0.56158  -4.69442   7.09958 ...  39.79258  47.61258  43.38158]\n",
      " [-26.3051  -19.7671   -4.6381  ...  64.8489   75.7459   57.2849 ]\n",
      " [-10.63602 -13.71302  -8.32802 ...  40.13298  44.74898  40.26198]]\n"
     ]
    }
   ],
   "source": [
    "# 作为基线的比例\n",
    "baseline_ratio = 0.1\n",
    "baseline_start = 0\n",
    "baseline_end = int(0.1 * nTime)\n",
    "\n",
    "x = deepcopy(standard_input_data_list[0][0])\n",
    "print(\"基线校正前 : \", x)\n",
    "x = baseline_correction(x, baseline_start, baseline_end)\n",
    "print(\"基线校正后 : \", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 带通滤波器 0.05Hz-40Hz\n",
    "lowcut = 0.05\n",
    "highcut = 40\n",
    "fs = fs_down\n",
    "order = 2\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order):\n",
    "    fa = 0.5 * fs\n",
    "    low = lowcut / fa\n",
    "    high = highcut / fa\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    ret = []\n",
    "    for line in data:\n",
    "        ret.append(filtfilt(b, a, line))\n",
    "    return np.array(ret)\n",
    "\n",
    "def iirnotch_filter(data, fs = 125, Q = 30, f_cut = 50.0):\n",
    "    ret = []\n",
    "    b, a = iirnotch(f_cut, Q, fs)\n",
    "    for line in data:\n",
    "        ret.append(lfilter(b,a, line))\n",
    "    return np.array(ret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "滤波前 :  [[ 11.42274  14.62774  20.65374 ...  33.08974  45.52574  27.96174]\n",
      " [ 23.91832  24.55932  23.91832 ...  38.27732  56.86632  42.50732]\n",
      " [ 14.37734  10.01834  14.24834 ...  37.71034  41.81234  32.58234]\n",
      " ...\n",
      " [  0.56158  -4.69442   7.09958 ...  39.79258  47.61258  43.38158]\n",
      " [-26.3051  -19.7671   -4.6381  ...  64.8489   75.7459   57.2849 ]\n",
      " [-10.63602 -13.71302  -8.32802 ...  40.13298  44.74898  40.26198]]\n",
      "滤波后 :  [[  1.2233674    5.31136275   9.38478621 ...  23.26321155  28.70143919\n",
      "   16.27871029]\n",
      " [  4.62252065   4.89188273   5.60975696 ...   7.82624006  23.85918253\n",
      "   12.7586109 ]\n",
      " [ -1.11384209  -4.53790032  -1.92974007 ...  11.24382281  13.61377979\n",
      "    5.92651266]\n",
      " ...\n",
      " [  0.54598765  -3.07385892   6.49437274 ...  -7.09438025  -0.34693384\n",
      "   -3.45731556]\n",
      " [-11.19020058  -4.24634911   8.3426814  ...  37.09434085  42.91027207\n",
      "   28.24304665]\n",
      " [ -4.26287477  -6.92866165  -2.46119829 ...   3.37859591   5.17940678\n",
      "    2.14527731]]\n"
     ]
    }
   ],
   "source": [
    "print(\"滤波前 : \", x)\n",
    "x = butter_bandpass_filter(x, lowcut, highcut, fs, order)\n",
    "x = iirnotch_filter(x)\n",
    "print(\"滤波后 : \", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.getcwd() + \"/lyh_data/Preprocessed_data\"\n",
    "\n",
    "def pre_processing(data, save_path, butter_order = 2):\n",
    "\n",
    "    for data_per_class in data:\n",
    "        for i, d in enumerate(data_per_class):\n",
    "            assert d.shape == (14, 500)\n",
    "            # data_per_class[i] = baseline_correction(data_per_class[i], baseline_start, baseline_end)\n",
    "            data_per_class[i] = butter_bandpass_filter(data_per_class[i], lowcut, highcut, fs, order)\n",
    "            data_per_class[i] = iirnotch_filter(data_per_class[i])\n",
    "    \n",
    "    # save\n",
    "    for i, d in enumerate(data):\n",
    "        data[i] = np.array(d)\n",
    "        path = save_path + \"/\" + str(i) + \".npy\"\n",
    "        np.save(path, data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "滤波后:  [[  1.2233674    5.31136274   9.3847862  ...  23.26321155  28.70143919\n",
      "   16.27871029]\n",
      " [  4.62252064   4.89188272   5.60975696 ...   7.82624006  23.85918253\n",
      "   12.7586109 ]\n",
      " [ -1.1138421   -4.53790033  -1.92974008 ...  11.24382281  13.61377978\n",
      "    5.92651266]\n",
      " ...\n",
      " [  0.54598764  -3.07385893   6.49437274 ...  -7.09438025  -0.34693384\n",
      "   -3.45731556]\n",
      " [-11.19020058  -4.24634911   8.3426814  ...  37.09434085  42.91027207\n",
      "   28.24304665]\n",
      " [ -4.26287477  -6.92866165  -2.46119829 ...   3.37859591   5.17940678\n",
      "    2.14527731]]\n"
     ]
    }
   ],
   "source": [
    "pre_processing(standard_input_data_list, save_path)\n",
    "print(\"滤波后: \", standard_input_data_list[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 特征提取\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 时域特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(standard_input_data_list), len(standard_input_data_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 14, 500)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_np = np.array(standard_input_data_list)\n",
    "dataset_np = dataset_np.reshape(-1, 14, 500)\n",
    "dataset_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = list(dataset_np)\n",
    "label_list = [0] * 300 + [1] * 300 + [2] * 300 + [3] * 300\n",
    "data_df = pd.DataFrame({'raw data': dataset_list, 'label': label_list})\n",
    "x = deepcopy(data_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算过零率\n",
    "\n",
    "def zero_crossing_rate(trials):\n",
    "    '''\n",
    "    计算一个trials(二维列表)各个通道的过零率, 返回一个list\n",
    "    '''\n",
    "\n",
    "    def compute(signal):\n",
    "        '''\n",
    "        计算一维信号的过零率\n",
    "        '''\n",
    "        crossings = np.where(np.diff(np.sign(signal)))[0]\n",
    "        zero_crossing_rate = len(crossings) / len(signal)\n",
    "        return zero_crossing_rate\n",
    "\n",
    "    ret = [compute(trials[i]) for i in range(len(trials))]\n",
    "    return np.array(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.168, 0.04 , 0.13 , 0.102, 0.198, 0.15 , 0.14 , 0.186, 0.126,\n",
       "        0.074, 0.106, 0.114, 0.062, 0.046]),\n",
       " array([0.15 , 0.088, 0.156, 0.084, 0.108, 0.112, 0.174, 0.184, 0.18 ,\n",
       "        0.136, 0.108, 0.184, 0.08 , 0.104]),\n",
       " array([0.138, 0.1  , 0.078, 0.07 , 0.05 , 0.154, 0.184, 0.09 , 0.172,\n",
       "        0.094, 0.12 , 0.124, 0.062, 0.112]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 同一类别的过零率差别大\n",
    "np.array(zero_crossing_rate(x['raw data'])),\\\n",
    "     np.array(zero_crossing_rate(data_df.iloc[1]['raw data'])),\\\n",
    "       np.array(zero_crossing_rate(data_df.iloc[2]['raw data'])  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.056, 0.084, 0.112, 0.106, 0.05 , 0.066, 0.062, 0.17 , 0.136,\n",
       "        0.076, 0.128, 0.138, 0.1  , 0.084]),\n",
       " array([0.106, 0.056, 0.098, 0.054, 0.028, 0.04 , 0.06 , 0.104, 0.076,\n",
       "        0.064, 0.032, 0.068, 0.046, 0.076]),\n",
       " array([0.012, 0.008, 0.016, 0.054, 0.026, 0.014, 0.018, 0.058, 0.05 ,\n",
       "        0.042, 0.054, 0.032, 0.034, 0.048]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 同一类别的过零率差别大\n",
    "np.array(zero_crossing_rate(data_df.iloc[398]['raw data'])),\\\n",
    "     np.array(zero_crossing_rate(data_df.iloc[399]['raw data'])),\\\n",
    "       np.array(zero_crossing_rate(data_df.iloc[400]['raw data'])  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算均值\n",
    "\n",
    "def calculate_channel_means(eeg_data):\n",
    "    \"\"\"\n",
    "    计算每个通道的均值。\n",
    "\n",
    "    参数：\n",
    "    - eeg_data: 二维 NumPy 数组，表示 EEG 数据，形状为 (n_channels, n_points)。\n",
    "\n",
    "    返回：\n",
    "    - channel_means: 一维 NumPy 数组，包含每个通道的均值。\n",
    "    \"\"\"\n",
    "    # 计算每个通道的均值，axis=2 表示在样本点上求均值\n",
    "    channel_means = np.mean(eeg_data, axis=1)\n",
    "\n",
    "    return channel_means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  5.79579612,  -6.37368547,  -1.11140977, -10.55618282,\n",
       "          2.01619029,  -1.79062357,  -1.40401937,   0.80076936,\n",
       "          3.03901338,   4.12667989,  11.40424161,  -0.07762688,\n",
       "         26.94610349,  -1.1365759 ]),\n",
       " array([ -7.05026265, -18.51313891,  -5.52623322,  -6.18110454,\n",
       "          5.75653787,  10.04083053,  -0.65157368,   0.92565608,\n",
       "         -1.22593159,  -4.27781215,   6.5364136 ,   3.26128473,\n",
       "          6.60940731,   5.42792899]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 同一类别的trials均值差别怎么这么大？\n",
    "calculate_channel_means(data_df.iloc[0]['raw data']), calculate_channel_means(data_df.iloc[1]['raw data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算标准差\n",
    "\n",
    "def calculate_channel_stds(eeg_data):\n",
    "    \"\"\"\n",
    "    计算每个通道的均值。\n",
    "\n",
    "    参数：\n",
    "    - eeg_data: 二维 NumPy 数组，表示 EEG 数据，形状为 (n_channels, n_points)。\n",
    "\n",
    "    返回：\n",
    "    - channel_means: 一维 NumPy 数组，包含每个通道的均值。\n",
    "    \"\"\"\n",
    "    # 计算每个通道的均值，axis=2 表示在样本点上求均值\n",
    "    channel_stds = np.std(eeg_data, axis=1)\n",
    "\n",
    "    return channel_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_first_order_diff(eeg_data):\n",
    "\n",
    "    first_order_diff = np.sum(np.abs(np.diff(eeg_data, axis=1)), axis=1) / (eeg_data.shape[1] - 1)\n",
    "\n",
    "    return first_order_diff\n",
    "\n",
    "def calculate_second_order_diff(eeg_data):\n",
    "    \n",
    "    second_order_diff = np.sum(np.abs(np.diff(eeg_data, n=2, axis=1)), axis=1) / (eeg_data.shape[1] - 2)\n",
    "\n",
    "    return second_order_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_domain_feature(eeg_dataset):\n",
    "    '''\n",
    "    提取时域特征\n",
    "    params:\n",
    "    - eeg_dataset: Dataframe类型\n",
    "    '''\n",
    "    zero_crossing_list = []\n",
    "    means_list = []\n",
    "    stds_list = []\n",
    "    first_order_diff_list = []\n",
    "    second_order_diff_list = []\n",
    "\n",
    "    for _, row in eeg_dataset.iterrows():\n",
    "        zero_crossing = zero_crossing_rate(row['raw data'])\n",
    "        zero_crossing_list.append(zero_crossing)\n",
    "        means = calculate_channel_means(row['raw data'])\n",
    "        means_list.append(means)\n",
    "        stds = calculate_channel_stds(row['raw data'])\n",
    "        stds_list.append(stds)\n",
    "        first_diff = calculate_first_order_diff(row['raw data'])\n",
    "        first_order_diff_list.append(first_diff)\n",
    "        second_diff = calculate_second_order_diff(row['raw data'])\n",
    "        second_order_diff_list.append(second_diff)\n",
    "    \n",
    "    nChannels = zero_crossing_list[0].shape[0]\n",
    "    for i in range(nChannels):\n",
    "        eeg_dataset[f'zero_crossing_rate_c{i+1}'] = np.array(zero_crossing_list)[:,i]\n",
    "        eeg_dataset[f'mean_c{i+1}'] = np.array(means_list)[:,i]\n",
    "        eeg_dataset[f'std_c{i+1}'] = np.array(stds_list)[:,i]\n",
    "        eeg_dataset[f'first_order_diff_c{i+1}'] = np.array(first_order_diff_list)[:,i]\n",
    "        eeg_dataset[f'second_order_diff_c{i+1}'] = np.array(second_order_diff_list)[:,i]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_domain_feature(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1200 entries, 0 to 1199\n",
      "Data columns (total 72 columns):\n",
      "raw data                  1200 non-null object\n",
      "label                     1200 non-null int64\n",
      "zero_crossing_rate_c1     1200 non-null float64\n",
      "mean_c1                   1200 non-null float64\n",
      "std_c1                    1200 non-null float64\n",
      "first_order_diff_c1       1200 non-null float64\n",
      "second_order_diff_c1      1200 non-null float64\n",
      "zero_crossing_rate_c2     1200 non-null float64\n",
      "mean_c2                   1200 non-null float64\n",
      "std_c2                    1200 non-null float64\n",
      "first_order_diff_c2       1200 non-null float64\n",
      "second_order_diff_c2      1200 non-null float64\n",
      "zero_crossing_rate_c3     1200 non-null float64\n",
      "mean_c3                   1200 non-null float64\n",
      "std_c3                    1200 non-null float64\n",
      "first_order_diff_c3       1200 non-null float64\n",
      "second_order_diff_c3      1200 non-null float64\n",
      "zero_crossing_rate_c4     1200 non-null float64\n",
      "mean_c4                   1200 non-null float64\n",
      "std_c4                    1200 non-null float64\n",
      "first_order_diff_c4       1200 non-null float64\n",
      "second_order_diff_c4      1200 non-null float64\n",
      "zero_crossing_rate_c5     1200 non-null float64\n",
      "mean_c5                   1200 non-null float64\n",
      "std_c5                    1200 non-null float64\n",
      "first_order_diff_c5       1200 non-null float64\n",
      "second_order_diff_c5      1200 non-null float64\n",
      "zero_crossing_rate_c6     1200 non-null float64\n",
      "mean_c6                   1200 non-null float64\n",
      "std_c6                    1200 non-null float64\n",
      "first_order_diff_c6       1200 non-null float64\n",
      "second_order_diff_c6      1200 non-null float64\n",
      "zero_crossing_rate_c7     1200 non-null float64\n",
      "mean_c7                   1200 non-null float64\n",
      "std_c7                    1200 non-null float64\n",
      "first_order_diff_c7       1200 non-null float64\n",
      "second_order_diff_c7      1200 non-null float64\n",
      "zero_crossing_rate_c8     1200 non-null float64\n",
      "mean_c8                   1200 non-null float64\n",
      "std_c8                    1200 non-null float64\n",
      "first_order_diff_c8       1200 non-null float64\n",
      "second_order_diff_c8      1200 non-null float64\n",
      "zero_crossing_rate_c9     1200 non-null float64\n",
      "mean_c9                   1200 non-null float64\n",
      "std_c9                    1200 non-null float64\n",
      "first_order_diff_c9       1200 non-null float64\n",
      "second_order_diff_c9      1200 non-null float64\n",
      "zero_crossing_rate_c10    1200 non-null float64\n",
      "mean_c10                  1200 non-null float64\n",
      "std_c10                   1200 non-null float64\n",
      "first_order_diff_c10      1200 non-null float64\n",
      "second_order_diff_c10     1200 non-null float64\n",
      "zero_crossing_rate_c11    1200 non-null float64\n",
      "mean_c11                  1200 non-null float64\n",
      "std_c11                   1200 non-null float64\n",
      "first_order_diff_c11      1200 non-null float64\n",
      "second_order_diff_c11     1200 non-null float64\n",
      "zero_crossing_rate_c12    1200 non-null float64\n",
      "mean_c12                  1200 non-null float64\n",
      "std_c12                   1200 non-null float64\n",
      "first_order_diff_c12      1200 non-null float64\n",
      "second_order_diff_c12     1200 non-null float64\n",
      "zero_crossing_rate_c13    1200 non-null float64\n",
      "mean_c13                  1200 non-null float64\n",
      "std_c13                   1200 non-null float64\n",
      "first_order_diff_c13      1200 non-null float64\n",
      "second_order_diff_c13     1200 non-null float64\n",
      "zero_crossing_rate_c14    1200 non-null float64\n",
      "mean_c14                  1200 non-null float64\n",
      "std_c14                   1200 non-null float64\n",
      "first_order_diff_c14      1200 non-null float64\n",
      "second_order_diff_c14     1200 non-null float64\n",
      "dtypes: float64(70), int64(1), object(1)\n",
      "memory usage: 675.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 频域特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import welch\n",
    "\n",
    "def five_band_energy(eeg_data, fs=125):\n",
    "    \n",
    "    energy = []\n",
    "    \n",
    "    for eeg_signal in eeg_data:\n",
    "        # 计算功率谱密度（PSD）\n",
    "        frequencies, psd = welch(eeg_signal, fs, nperseg=1024)\n",
    "\n",
    "        # 定义频带边界\n",
    "        delta_band = (0.5, 4)\n",
    "        theta_band = (4, 8)\n",
    "        alpha_band = (8, 14)\n",
    "        beta_band = (14, 30)\n",
    "        gamma_band = (30, 60)\n",
    "\n",
    "        # 计算每个频带内的能量\n",
    "        energy.append(np.trapz(psd[(frequencies >= delta_band[0]) & (frequencies <= delta_band[1])], \\\n",
    "            frequencies[(frequencies >= delta_band[0]) & (frequencies <= delta_band[1])]))\n",
    "        energy.append(np.trapz(psd[(frequencies >= theta_band[0]) & (frequencies <= theta_band[1])], \\\n",
    "            frequencies[(frequencies >= theta_band[0]) & (frequencies <= theta_band[1])]))\n",
    "        energy.append(np.trapz(psd[(frequencies >= alpha_band[0]) & (frequencies <= alpha_band[1])], \\\n",
    "            frequencies[(frequencies >= alpha_band[0]) & (frequencies <= alpha_band[1])]))\n",
    "        energy.append(np.trapz(psd[(frequencies >= beta_band[0]) & (frequencies <= beta_band[1])], \\\n",
    "            frequencies[(frequencies >= beta_band[0]) & (frequencies <= beta_band[1])]))\n",
    "        energy.append(np.trapz(psd[(frequencies >= gamma_band[0]) & (frequencies <= gamma_band[1])], \\\n",
    "            frequencies[(frequencies >= gamma_band[0]) & (frequencies <= gamma_band[1])]))\n",
    "\n",
    "    return np.array(energy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_domain_feature(eeg_dataset):\n",
    "    '''\n",
    "    提取时域特征\n",
    "\n",
    "    参数:\n",
    "    - eeg_dataset: Dataframe类型\n",
    "    '''\n",
    "    \n",
    "    energy = []\n",
    "\n",
    "    for _, row in eeg_dataset.iterrows():\n",
    "        \n",
    "        energy.append(five_band_energy(row['raw data']))\n",
    "\n",
    "    freq_names = ['delta', 'theta', 'alpha', 'beta', 'gamma']\n",
    "    nFreqs = len(freq_names)\n",
    "    nChannels = energy[0].shape[0] // nFreqs\n",
    "    \n",
    "    for i in range(nChannels):\n",
    "        for j in range(nFreqs):\n",
    "            eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_domain_feature(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1200 entries, 0 to 1199\n",
      "Columns: 142 entries, raw data to gamma_c14\n",
      "dtypes: float64(140), int64(1), object(1)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.getcwd() + \"/lyh_data/Feature_extracted/lyh_data.csv\"\n",
    "data_df.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造数据集\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 打乱数据集\n",
    "shuffled_df = data_df.sample(frac=1, random_state=42)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "train_df, test_df = train_test_split(shuffled_df, test_size=0.2, random_state=42)\n",
    "y_train = train_df['label']\n",
    "X_train = train_df.drop(columns=['label', 'raw data'])\n",
    "y_test = test_df['label']\n",
    "X_test = test_df.drop(columns=['label', 'raw data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((960, 140), (960,), (240, 140), (240,))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "               importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
       "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent='warn',\n",
       "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgm = LGBMClassifier()\n",
    "lgm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.9125)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 手工特征提取+未调参LightGBM\n",
    "y_train_pred = lgm.predict(X_train)\n",
    "acc_train = (y_train == y_train_pred).sum() / len(y_train)\n",
    "\n",
    "y_test_pred = lgm.predict(X_test)\n",
    "acc_test = (y_test == y_test_pred).sum() / len(y_test)\n",
    "acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.4875)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 原始数据进行训练\n",
    "\n",
    "shuffled_df = data_df.sample(frac=1, random_state=42)\n",
    "train_df, test_df = train_test_split(shuffled_df, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = np.array(train_df['raw data'])\n",
    "X_test = np.array(test_df['raw data'])\n",
    "X_train = np.vstack(X_train).reshape(-1, 14, 500)\n",
    "X_test = np.vstack(X_test).reshape(-1, 14, 500)\n",
    "X_train = X_train.reshape(-1, 14 * 500)\n",
    "X_test = X_test.reshape(-1, 14 * 500)\n",
    "y_train = np.array(train_df['label'])\n",
    "y_test = np.array(test_df['label'])\n",
    "\n",
    "lgm = LGBMClassifier()\n",
    "lgm.fit(X_train, y_train)\n",
    "\n",
    "# 原始特征+未调参LightGBM\n",
    "y_train_pred = lgm.predict(X_train)\n",
    "acc_train = (y_train == y_train_pred).sum() / len(y_train)\n",
    "\n",
    "y_test_pred = lgm.predict(X_test)\n",
    "acc_test = (y_test == y_test_pred).sum() / len(y_test)\n",
    "acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.4625)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "# 原始数据进行训练\n",
    "\n",
    "shuffled_df = data_df.sample(frac=1, random_state=42)\n",
    "train_df, test_df = train_test_split(shuffled_df, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = np.array(train_df['raw data'])\n",
    "X_test = np.array(test_df['raw data'])\n",
    "X_train = np.vstack(X_train).reshape(-1, 14, 500)\n",
    "X_test = np.vstack(X_test).reshape(-1, 14, 500)\n",
    "X_train = X_train.reshape(-1, 14 * 500)\n",
    "X_test = X_test.reshape(-1, 14 * 500)\n",
    "y_train = np.array(train_df['label'])\n",
    "y_test = np.array(test_df['label'])\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# 原始特征+未调参xgboost\n",
    "y_train_pred = xgb.predict(X_train)\n",
    "acc_train = (y_train == y_train_pred).sum() / len(y_train)\n",
    "\n",
    "y_test_pred = xgb.predict(X_test)\n",
    "acc_test = (y_test == y_test_pred).sum() / len(y_test)\n",
    "acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.925)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构造数据集\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 打乱数据集\n",
    "shuffled_df = data_df.sample(frac=1, random_state=42)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "train_df, test_df = train_test_split(shuffled_df, test_size=0.2, random_state=42)\n",
    "y_train = train_df['label']\n",
    "X_train = train_df.drop(columns=['label', 'raw data'])\n",
    "y_test = test_df['label']\n",
    "X_test = test_df.drop(columns=['label', 'raw data'])\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# 手工特征提取+未调参XGBoost\n",
    "y_train_pred = xgb.predict(X_train)\n",
    "# acc_train = (y_train == y_train_pred).sum() / len(y_train)\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "y_test_pred = xgb.predict(X_test)\n",
    "# acc_test = (y_test == y_test_pred).sum() / len(y_test)\n",
    "acc_test = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "acc_train, acc_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Parameters:\n",
      "objective: multi:softprob\n",
      "use_label_encoder: False\n",
      "base_score: 0.5\n",
      "booster: gbtree\n",
      "callbacks: None\n",
      "colsample_bylevel: 1\n",
      "colsample_bynode: 1\n",
      "colsample_bytree: 1\n",
      "early_stopping_rounds: None\n",
      "enable_categorical: False\n",
      "eval_metric: None\n",
      "gamma: 0\n",
      "gpu_id: -1\n",
      "grow_policy: depthwise\n",
      "importance_type: None\n",
      "interaction_constraints: \n",
      "learning_rate: 0.300000012\n",
      "max_bin: 256\n",
      "max_cat_to_onehot: 4\n",
      "max_delta_step: 0\n",
      "max_depth: 6\n",
      "max_leaves: 0\n",
      "min_child_weight: 1\n",
      "missing: nan\n",
      "monotone_constraints: ()\n",
      "n_estimators: 100\n",
      "n_jobs: 0\n",
      "num_parallel_tree: 1\n",
      "predictor: auto\n",
      "random_state: 0\n",
      "reg_alpha: 0\n",
      "reg_lambda: 1\n",
      "sampling_method: uniform\n",
      "scale_pos_weight: None\n",
      "subsample: 1\n",
      "tree_method: exact\n",
      "validate_parameters: 1\n",
      "verbosity: None\n"
     ]
    }
   ],
   "source": [
    "default_params = xgb.get_params()\n",
    "# 打印参数\n",
    "print(\"Default Parameters:\")\n",
    "for param, value in default_params.items():\n",
    "    print(f\"{param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'learning_rate': 0.35, 'n_estimators': 90}\n",
      "best score:  0.8991666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 1. 确定learning_rate和n_estimators\n",
    "param_test1 = {\n",
    " 'learning_rate': [0.2, 0.25 ,0.300000012, 0.35],\n",
    " 'n_estimators': [90, 95, 100, 105]\n",
    "}\n",
    "\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier(), \\\n",
    "    param_grid = param_test1, scoring='accuracy', cv=3)\n",
    "gsearch1.fit(pd.concat([X_train, X_test],axis=0), pd.concat([y_train,y_test],axis=0))\n",
    "print(f\"best params: {gsearch1.best_params_}\\nbest score:  {gsearch1.best_score_}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.9083333333333333)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'learning_rate': 0.35,\n",
    "    'n_estimators': 90\n",
    "}\n",
    "\n",
    "model1 = XGBClassifier(**params)\n",
    "model1.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = model1.predict(X_train)\n",
    "acc_train = (y_train == y_train_pred).sum() / len(y_train)\n",
    "\n",
    "y_test_pred = model1.predict(X_test)\n",
    "acc_test = (y_test == y_test_pred).sum() / len(y_test)\n",
    "\n",
    "acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'max_depth': 9, 'min_child_weight': 5}\n",
      "best score:  0.9033333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 2. 确定max_depth和min_child_weight\n",
    "params = {\n",
    "    'learning_rate': 0.35,\n",
    "    'n_estimators': 90\n",
    "}\n",
    "\n",
    "param_test2 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}\n",
    "\n",
    "gsearch2 = GridSearchCV(estimator = XGBClassifier(**param), \\\n",
    "    param_grid = param_test2, scoring='accuracy', cv=3)\n",
    "gsearch2.fit(pd.concat([X_train, X_test],axis=0), pd.concat([y_train,y_test],axis=0))\n",
    "print(f\"best params: {gsearch2.best_params_}\\nbest score:  {gsearch2.best_score_}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.925)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'learning_rate': 0.35,\n",
    "    'n_estimators': 90,\n",
    "    'max_depth': 9, \n",
    "    'min_child_weight': 5\n",
    "}\n",
    "\n",
    "model2 = XGBClassifier(**params)\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = model2.predict(X_train)\n",
    "acc_train = (y_train == y_train_pred).sum() / len(y_train)\n",
    "\n",
    "y_test_pred = model2.predict(X_test)\n",
    "acc_test = (y_test == y_test_pred).sum() / len(y_test)\n",
    "\n",
    "acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'gamma': 0.0}\n",
      "best score:  0.8916666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 3. 确定gamma\n",
    "params = {\n",
    "    'learning_rate': 0.35,\n",
    "    'n_estimators': 90,\n",
    "    'max_depth': 9, \n",
    "    'min_child_weight': 5\n",
    "}\n",
    "\n",
    "param_test3 = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "\n",
    "gsearch3 = GridSearchCV(estimator = XGBClassifier(**param), \\\n",
    "    param_grid = param_test3, scoring='accuracy', cv=3)\n",
    "gsearch3.fit(pd.concat([X_train, X_test],axis=0), pd.concat([y_train,y_test],axis=0))\n",
    "print(f\"best params: {gsearch3.best_params_}\\nbest score:  {gsearch3.best_score_}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'reg_alpha': 0.01}\n",
      "best score:  0.8966666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 4. 确定reg_alpha\n",
    "param = {\n",
    "    'learning_rate': 0.3,\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 1,\n",
    "    'gamma': 0.0\n",
    "}\n",
    "\n",
    "param_test4 = {\n",
    " 'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05]\n",
    "}\n",
    "\n",
    "gsearch4 = GridSearchCV(estimator = XGBClassifier(**param), \\\n",
    "    param_grid = param_test4, scoring='accuracy', cv=3)\n",
    "gsearch4.fit(pd.concat([X_train, X_test],axis=0), pd.concat([y_train,y_test],axis=0))\n",
    "print(f\"best params: {gsearch4.best_params_}\\nbest score:  {gsearch4.best_score_}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.925)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'learning_rate': 0.3,\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 9, \n",
    "    'min_child_weight': 5,\n",
    "    'reg_alpha': 0.01\n",
    "}\n",
    "\n",
    "model4 = XGBClassifier(**params)\n",
    "model4.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = model4.predict(X_train)\n",
    "acc_train = (y_train == y_train_pred).sum() / len(y_train)\n",
    "\n",
    "y_test_pred = model4.predict(X_test)\n",
    "acc_test = (y_test == y_test_pred).sum() / len(y_test)\n",
    "\n",
    "acc_train, acc_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
