{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2432"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(os.getcwd() + \"/data/train/nothing\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件夹 '/mnt/workspace/Baseline/data/train/nothing' 的总大小：\n",
      "9951349 字节\n",
      "9718.11 KB\n",
      "9.49 MB\n",
      "0.01 GB\n"
     ]
    }
   ],
   "source": [
    "def get_folder_size(folder_path):\n",
    "    total_size = 0\n",
    "    try:\n",
    "        # 遍历文件夹中的所有文件和子文件夹\n",
    "        for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "            for filename in filenames:\n",
    "                filepath = os.path.join(dirpath, filename)\n",
    "                total_size += os.path.getsize(filepath)\n",
    "\n",
    "        # 将字节数转换为更大单位\n",
    "        total_size_kb = total_size / 1024.0\n",
    "        total_size_mb = total_size_kb / 1024.0\n",
    "        total_size_gb = total_size_mb / 1024.0\n",
    "\n",
    "        print(f\"文件夹 '{folder_path}' 的总大小：\")\n",
    "        print(f\"{total_size} 字节\")\n",
    "        print(f\"{total_size_kb:.2f} KB\")\n",
    "        print(f\"{total_size_mb:.2f} MB\")\n",
    "        print(f\"{total_size_gb:.2f} GB\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"文件夹 '{folder_path}' 不存在。\")\n",
    "\n",
    "# 替换为你要检查的文件夹路径\n",
    "folder_path_to_check = os.getcwd() + \"/data/train/nothing\"\n",
    "\n",
    "# 调用函数\n",
    "get_folder_size(folder_path_to_check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded nothing_processed.npy: Shape=(15, 300000), Dtype=float64\n"
     ]
    }
   ],
   "source": [
    "from load_lyh_data import *\n",
    "raw_data_dict = read_folder_npy_data(selected_file=[\"nothing_processed.npy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial0 = raw_data_dict[\"nothing_processed.npy\"][0:-2,0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial1 = raw_data_dict[\"nothing_processed.npy\"][0:-2, 1000:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.01442085, -1.56067872, -0.61960121, -1.33475331, -0.1442223 ,\n",
       "        -0.11273622,  0.32821187, -0.00432148,  0.68648067,  1.03092712,\n",
       "         1.53480116, -0.03937722,  2.97759998]),\n",
       " array([-0.44633239,  3.17614217, -0.49152672,  1.46486502,  0.68273147,\n",
       "        -0.06183269, -0.68817793, -1.48605677, -1.58720262, -2.11189917,\n",
       "        -3.03902474, -1.33781629, -4.38515012]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial0.mean(axis=1), trial1.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T12:30:03.149193Z",
     "iopub.status.busy": "2024-02-28T12:30:03.148787Z",
     "iopub.status.idle": "2024-02-28T12:30:16.391047Z",
     "shell.execute_reply": "2024-02-28T12:30:16.390437Z",
     "shell.execute_reply.started": "2024-02-28T12:30:03.149168Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded nothing1_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded nothing2_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded nothing3_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded left1_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded left2_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded left3_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded right1_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded right2_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded right3_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded leg1_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded leg2_orginal.npy: Shape=(100,), Dtype=object\n",
      "Loaded leg3_orginal.npy: Shape=(100,), Dtype=object\n",
      "时域提取：\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1200 entries, 0 to 1199\n",
      "Data columns (total 72 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   raw data                1200 non-null   object \n",
      " 1   label                   1200 non-null   int64  \n",
      " 2   zero_crossing_rate_c1   1200 non-null   float64\n",
      " 3   mean_c1                 1200 non-null   float64\n",
      " 4   std_c1                  1200 non-null   float64\n",
      " 5   first_order_diff_c1     1200 non-null   float64\n",
      " 6   second_order_diff_c1    1200 non-null   float64\n",
      " 7   zero_crossing_rate_c2   1200 non-null   float64\n",
      " 8   mean_c2                 1200 non-null   float64\n",
      " 9   std_c2                  1200 non-null   float64\n",
      " 10  first_order_diff_c2     1200 non-null   float64\n",
      " 11  second_order_diff_c2    1200 non-null   float64\n",
      " 12  zero_crossing_rate_c3   1200 non-null   float64\n",
      " 13  mean_c3                 1200 non-null   float64\n",
      " 14  std_c3                  1200 non-null   float64\n",
      " 15  first_order_diff_c3     1200 non-null   float64\n",
      " 16  second_order_diff_c3    1200 non-null   float64\n",
      " 17  zero_crossing_rate_c4   1200 non-null   float64\n",
      " 18  mean_c4                 1200 non-null   float64\n",
      " 19  std_c4                  1200 non-null   float64\n",
      " 20  first_order_diff_c4     1200 non-null   float64\n",
      " 21  second_order_diff_c4    1200 non-null   float64\n",
      " 22  zero_crossing_rate_c5   1200 non-null   float64\n",
      " 23  mean_c5                 1200 non-null   float64\n",
      " 24  std_c5                  1200 non-null   float64\n",
      " 25  first_order_diff_c5     1200 non-null   float64\n",
      " 26  second_order_diff_c5    1200 non-null   float64\n",
      " 27  zero_crossing_rate_c6   1200 non-null   float64\n",
      " 28  mean_c6                 1200 non-null   float64\n",
      " 29  std_c6                  1200 non-null   float64\n",
      " 30  first_order_diff_c6     1200 non-null   float64\n",
      " 31  second_order_diff_c6    1200 non-null   float64\n",
      " 32  zero_crossing_rate_c7   1200 non-null   float64\n",
      " 33  mean_c7                 1200 non-null   float64\n",
      " 34  std_c7                  1200 non-null   float64\n",
      " 35  first_order_diff_c7     1200 non-null   float64\n",
      " 36  second_order_diff_c7    1200 non-null   float64\n",
      " 37  zero_crossing_rate_c8   1200 non-null   float64\n",
      " 38  mean_c8                 1200 non-null   float64\n",
      " 39  std_c8                  1200 non-null   float64\n",
      " 40  first_order_diff_c8     1200 non-null   float64\n",
      " 41  second_order_diff_c8    1200 non-null   float64\n",
      " 42  zero_crossing_rate_c9   1200 non-null   float64\n",
      " 43  mean_c9                 1200 non-null   float64\n",
      " 44  std_c9                  1200 non-null   float64\n",
      " 45  first_order_diff_c9     1200 non-null   float64\n",
      " 46  second_order_diff_c9    1200 non-null   float64\n",
      " 47  zero_crossing_rate_c10  1200 non-null   float64\n",
      " 48  mean_c10                1200 non-null   float64\n",
      " 49  std_c10                 1200 non-null   float64\n",
      " 50  first_order_diff_c10    1200 non-null   float64\n",
      " 51  second_order_diff_c10   1200 non-null   float64\n",
      " 52  zero_crossing_rate_c11  1200 non-null   float64\n",
      " 53  mean_c11                1200 non-null   float64\n",
      " 54  std_c11                 1200 non-null   float64\n",
      " 55  first_order_diff_c11    1200 non-null   float64\n",
      " 56  second_order_diff_c11   1200 non-null   float64\n",
      " 57  zero_crossing_rate_c12  1200 non-null   float64\n",
      " 58  mean_c12                1200 non-null   float64\n",
      " 59  std_c12                 1200 non-null   float64\n",
      " 60  first_order_diff_c12    1200 non-null   float64\n",
      " 61  second_order_diff_c12   1200 non-null   float64\n",
      " 62  zero_crossing_rate_c13  1200 non-null   float64\n",
      " 63  mean_c13                1200 non-null   float64\n",
      " 64  std_c13                 1200 non-null   float64\n",
      " 65  first_order_diff_c13    1200 non-null   float64\n",
      " 66  second_order_diff_c13   1200 non-null   float64\n",
      " 67  zero_crossing_rate_c14  1200 non-null   float64\n",
      " 68  mean_c14                1200 non-null   float64\n",
      " 69  std_c14                 1200 non-null   float64\n",
      " 70  first_order_diff_c14    1200 non-null   float64\n",
      " 71  second_order_diff_c14   1200 non-null   float64\n",
      "dtypes: float64(70), int64(1), object(1)\n",
      "memory usage: 675.1+ KB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pai/lib/python3.11/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 1024 is greater than input length  = 500, using nperseg = 500\n",
      "  warnings.warn('nperseg = {0:d} is greater than input length '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "频域提取：\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1200 entries, 0 to 1199\n",
      "Columns: 142 entries, raw data to gamma_c14\n",
      "dtypes: float64(140), int64(1), object(1)\n",
      "memory usage: 1.3+ MB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n",
      "/mnt/workspace/Baseline/feature_engineering.py:282: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eeg_dataset[f'{freq_names[j]}_c{i+1}'] = np.array(energy)[:,i*nFreqs+j]\n"
     ]
    }
   ],
   "source": [
    "from feature_engineering import *\n",
    "from load_lyh_data import  *\n",
    "\n",
    "raw_data_dict = read_folder_npy_data()\n",
    "standard_input_data_list = pre_preparation(raw_data_dict, config)\n",
    "preprocessed_data = pre_processing(standard_input_data_list, config)\n",
    "dataset_np = np.array(preprocessed_data)\n",
    "dataset_np = dataset_np.reshape(-1, 14, 500)\n",
    "label_list = [0] * 300 + [1] * 300 + [2] * 300 + [3] * 300\n",
    "data_df = pd.DataFrame({'raw data': list(dataset_np), 'label': label_list})\n",
    "data_df = time_domain_feature(data_df)\n",
    "print('时域提取：')\n",
    "print(data_df.info())\n",
    "data_df = freq_domain_feature(data_df)\n",
    "print('频域提取：')\n",
    "print(data_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-02-28T12:30:22.806736Z",
     "iopub.status.busy": "2024-02-28T12:30:22.806293Z",
     "iopub.status.idle": "2024-02-28T12:30:23.949218Z",
     "shell.execute_reply": "2024-02-28T12:30:23.948552Z",
     "shell.execute_reply.started": "2024-02-28T12:30:22.806706Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001760 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33576\n",
      "[LightGBM] [Info] Number of data points in the train set: 960, number of used features: 140\n",
      "[LightGBM] [Info] Start training from score -1.382136\n",
      "[LightGBM] [Info] Start training from score -1.365675\n",
      "[LightGBM] [Info] Start training from score -1.361602\n",
      "[LightGBM] [Info] Start training from score -1.437588\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0, 0.9125)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "# 构造数据集\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 打乱数据集\n",
    "shuffled_df = data_df.sample(frac=1, random_state=42)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "train_df, test_df = train_test_split(shuffled_df, test_size=0.2, random_state=42)\n",
    "y_train = train_df['label']\n",
    "X_train = train_df.drop(columns=['label', 'raw data'])\n",
    "y_test = test_df['label']\n",
    "X_test = test_df.drop(columns=['label', 'raw data'])\n",
    "\n",
    "\n",
    "lgm = LGBMClassifier()\n",
    "lgm.fit(X_train, y_train)\n",
    "\n",
    "# 手工特征提取+未调参LightGBM\n",
    "y_train_pred = lgm.predict(X_train)\n",
    "acc_train = (y_train == y_train_pred).sum() / len(y_train)\n",
    "\n",
    "y_test_pred = lgm.predict(X_test)\n",
    "acc_test = (y_test == y_test_pred).sum() / len(y_test)\n",
    "acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def test(data):\n",
    "    data = data[::2].copy()\n",
    "    return data\n",
    "\n",
    "d = np.array([1, 2, 3, 4])\n",
    "e = test(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e[0] = 2\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
